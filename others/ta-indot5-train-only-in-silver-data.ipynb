{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc6c19b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T10:38:48.222254Z",
     "iopub.status.busy": "2022-02-10T10:38:48.221429Z",
     "iopub.status.idle": "2022-02-10T10:39:23.296487Z",
     "shell.execute_reply": "2022-02-10T10:39:23.295495Z",
     "shell.execute_reply.started": "2022-02-10T09:30:34.627139Z"
    },
    "papermill": {
     "duration": 35.092587,
     "end_time": "2022-02-10T10:39:23.296660",
     "exception": false,
     "start_time": "2022-02-10T10:38:48.204073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.12.5)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\r\n",
      "Collecting indobenchmark-toolkit==0.0.4\r\n",
      "  Downloading indobenchmark_toolkit-0.0.4-py3-none-any.whl (8.0 kB)\r\n",
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\r\n",
      "     |████████████████████████████████| 90 kB 304 kB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from indobenchmark-toolkit==0.0.4) (1.9.1)\r\n",
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "     |████████████████████████████████| 1.2 MB 1.1 MB/s            \r\n",
      "\u001b[?25hCollecting datasets==1.4.1\r\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\r\n",
      "     |████████████████████████████████| 186 kB 14.3 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (6.0.1)\r\n",
      "Collecting tqdm<4.50.0,>=4.27\r\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\r\n",
      "     |████████████████████████████████| 69 kB 5.5 MB/s             \r\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.26.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.70.12.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.11.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.2)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (4.10.0)\r\n",
      "Collecting huggingface-hub==0.0.2\r\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.3.4)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.3.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.19.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.2->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 14.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 55.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 60.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 59.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 59.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\r\n",
      "     |████████████████████████████████| 3.3 MB 50.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 58.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 51.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 58.6 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\r\n",
      "  Downloading transformers-4.12.1-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 55.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 56.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 30.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 48.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.1-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 55.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 60.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 52.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 52.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 52.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 49.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 42.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 52.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 29.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 55.5 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 55.1 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 51.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 46.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\r\n",
      "     |████████████████████████████████| 2.2 MB 49.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\r\n",
      "     |████████████████████████████████| 2.3 MB 60.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\r\n",
      "     |████████████████████████████████| 2.1 MB 56.2 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.4)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.8.9)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.3.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.26.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.1)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.8)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7.1->indobenchmark-toolkit==0.0.4) (3.10.0.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.6.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.8.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.3)\r\n",
      "Installing collected packages: tqdm, huggingface-hub, transformers, sentencepiece, datasets, sacrebleu, indobenchmark-toolkit\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.62.3\r\n",
      "    Uninstalling tqdm-4.62.3:\r\n",
      "      Successfully uninstalled tqdm-4.62.3\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.1.2\r\n",
      "    Uninstalling huggingface-hub-0.1.2:\r\n",
      "      Successfully uninstalled huggingface-hub-0.1.2\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.12.5\r\n",
      "    Uninstalling transformers-4.12.5:\r\n",
      "      Successfully uninstalled transformers-4.12.5\r\n",
      "  Attempting uninstall: sentencepiece\r\n",
      "    Found existing installation: sentencepiece 0.1.96\r\n",
      "    Uninstalling sentencepiece-0.1.96:\r\n",
      "      Successfully uninstalled sentencepiece-0.1.96\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 1.17.0\r\n",
      "    Uninstalling datasets-1.17.0:\r\n",
      "      Successfully uninstalled datasets-1.17.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "beatrix-jupyterlab 3.1.4 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "tsfresh 0.19.0 requires statsmodels>=0.13, but you have statsmodels 0.12.2 which is incompatible.\r\n",
      "cached-path 0.3.2 requires huggingface-hub<0.2.0,>=0.0.12, but you have huggingface-hub 0.0.2 which is incompatible.\r\n",
      "cached-path 0.3.2 requires tqdm<4.63,>=4.62, but you have tqdm 4.49.0 which is incompatible.\r\n",
      "allennlp 2.8.0 requires huggingface-hub>=0.0.16, but you have huggingface-hub 0.0.2 which is incompatible.\u001b[0m\r\n",
      "Successfully installed datasets-1.4.1 huggingface-hub-0.0.2 indobenchmark-toolkit-0.0.4 sacrebleu-2.0.0 sentencepiece-0.1.95 tqdm-4.49.0 transformers-4.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece indobenchmark-toolkit==0.0.4 sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e67c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T10:39:23.425147Z",
     "iopub.status.busy": "2022-02-10T10:39:23.424272Z",
     "iopub.status.idle": "2022-02-10T10:39:27.049701Z",
     "shell.execute_reply": "2022-02-10T10:39:27.049128Z",
     "shell.execute_reply.started": "2022-02-10T09:31:01.641694Z"
    },
    "papermill": {
     "duration": 3.693494,
     "end_time": "2022-02-10T10:39:27.049896",
     "exception": false,
     "start_time": "2022-02-10T10:39:23.356402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-to-text-indonesia'...\r\n",
      "remote: Enumerating objects: 1422, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (1422/1422), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (930/930), done.\u001b[K\r\n",
      "remote: Total 1422 (delta 562), reused 1169 (delta 314), pack-reused 0\u001b[K\r\n",
      "Receiving objects: 100% (1422/1422), 1.71 MiB | 1.89 MiB/s, done.\r\n",
      "Resolving deltas: 100% (562/562), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-to-text-indonesia.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072b987a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T10:39:27.187984Z",
     "iopub.status.busy": "2022-02-10T10:39:27.187209Z",
     "iopub.status.idle": "2022-02-10T10:39:40.362199Z",
     "shell.execute_reply": "2022-02-10T10:39:40.361474Z",
     "shell.execute_reply.started": "2022-02-10T09:31:03.826604Z"
    },
    "papermill": {
     "duration": 13.24603,
     "end_time": "2022-02-10T10:39:40.362338",
     "exception": false,
     "start_time": "2022-02-10T10:39:27.116308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-indo-dataset'...\r\n",
      "remote: Enumerating objects: 57, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (57/57), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\r\n",
      "remote: Total 57 (delta 12), reused 53 (delta 11), pack-reused 0\u001b[K\r\n",
      "Unpacking objects: 100% (57/57), 55.48 MiB | 6.93 MiB/s, done.\r\n",
      "Updating files: 100% (50/50), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-indo-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d45ff630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T10:39:40.521608Z",
     "iopub.status.busy": "2022-02-10T10:39:40.521045Z",
     "iopub.status.idle": "2022-02-10T10:39:40.525238Z",
     "shell.execute_reply": "2022-02-10T10:39:40.525605Z",
     "shell.execute_reply.started": "2022-02-10T09:31:15.356852Z"
    },
    "papermill": {
     "duration": 0.086278,
     "end_time": "2022-02-10T10:39:40.525755",
     "exception": false,
     "start_time": "2022-02-10T10:39:40.439477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/train\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/train\n",
    "# %cd /kaggle/working/amr-indo-dataset\n",
    "# !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92b9905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T10:39:40.678717Z",
     "iopub.status.busy": "2022-02-10T10:39:40.677978Z",
     "iopub.status.idle": "2022-02-10T16:24:23.368761Z",
     "shell.execute_reply": "2022-02-10T16:24:23.369275Z"
    },
    "papermill": {
     "duration": 20682.770525,
     "end_time": "2022-02-10T16:24:23.369470",
     "exception": false,
     "start_time": "2022-02-10T10:39:40.598945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing method linearized_penman\r\n",
      "silver data folder ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/preprocessed_silver_data/train.amr.txt', result_sent_path='data/preprocessed_silver_data/train.sent.txt', source_file_path=None, source_folder_path='../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news')\r\n",
      "100%|███████████████████████████████| 268262/268262 [00:01<00:00, 141434.99it/s]\r\n",
      "100%|███████████████████████████████| 268572/268572 [00:01<00:00, 138048.74it/s]\r\n",
      "100%|███████████████████████████████| 266966/266966 [00:02<00:00, 126906.07it/s]\r\n",
      "100%|███████████████████████████████| 263263/263263 [00:01<00:00, 137236.86it/s]\r\n",
      "100%|███████████████████████████████| 269258/269258 [00:01<00:00, 142308.67it/s]\r\n",
      "100%|███████████████████████████████| 264330/264330 [00:01<00:00, 146119.23it/s]\r\n",
      "100%|███████████████████████████████| 263508/263508 [00:01<00:00, 144889.89it/s]\r\n",
      "100%|███████████████████████████████| 264253/264253 [00:02<00:00, 126775.52it/s]\r\n",
      "100%|███████████████████████████████| 270251/270251 [00:01<00:00, 139391.50it/s]\r\n",
      "100%|███████████████████████████████| 265325/265325 [00:01<00:00, 145576.54it/s]\r\n",
      "total: 154892 pair sent-amr\r\n",
      "Running on the GPU\r\n",
      "Downloading: 100%|████████████████████████████| 777k/777k [00:00<00:00, 918kB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 696/696 [00:00<00:00, 535kB/s]\r\n",
      "Downloading: 100%|███████████████████████████| 990M/990M [00:58<00:00, 16.8MB/s]\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  154892\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  38723\r\n",
      "(Epoch 1) TRAIN LOSS:1.2192 LR:0.00003000: 100%|█| 38723/38723 [1:52:59<00:00,  \r\n",
      "(Epoch 1) DEV LOSS:2.6641 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 29.23it/s]\r\n",
      "bleu score on dev:  25.15506380743694\r\n",
      "(Epoch 2) TRAIN LOSS:0.7401 LR:0.00003000: 100%|█| 38723/38723 [1:54:28<00:00,  \r\n",
      "(Epoch 2) DEV LOSS:2.6302 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 30.05it/s]\r\n",
      "bleu score on dev:  23.73374267672406\r\n",
      "(Epoch 3) TRAIN LOSS:0.6344 LR:0.00003000: 100%|█| 38723/38723 [1:54:58<00:00,  \r\n",
      "(Epoch 3) DEV LOSS:2.6936 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 26.64it/s]\r\n",
      "bleu score on dev:  26.465272124395764\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:19<00:00,  3.97it/s]\r\n",
      "sample:  ilham meniup balon. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah. ---- saya mengetik makalah\r\n",
      "sample:  angga anak ajaib. ---- angga anak ajaib\r\n",
      "sample:  ibu saya orang dosen. ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  35.66394591669426\r\n",
      "saya mengetik makalah.\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x train_indoT5_on_silver_data.sh\n",
    "!./train_indoT5_on_silver_data.sh linearized_penman ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b244d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T16:26:03.320887Z",
     "iopub.status.busy": "2022-02-10T16:26:03.319950Z",
     "iopub.status.idle": "2022-02-10T16:26:03.323792Z",
     "shell.execute_reply": "2022-02-10T16:26:03.323324Z"
    },
    "papermill": {
     "duration": 49.91611,
     "end_time": "2022-02-10T16:26:03.323914",
     "exception": false,
     "start_time": "2022-02-10T16:25:13.407804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/evaluate\n"
     ]
    }
   ],
   "source": [
    "%cd ../evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e743cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T16:27:43.056043Z",
     "iopub.status.busy": "2022-02-10T16:27:43.055261Z",
     "iopub.status.idle": "2022-02-10T16:27:43.706989Z",
     "shell.execute_reply": "2022-02-10T16:27:43.707448Z"
    },
    "papermill": {
     "duration": 50.537188,
     "end_time": "2022-02-10T16:27:43.707611",
     "exception": false,
     "start_time": "2022-02-10T16:26:53.170423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x evaluate_indoT5_to_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df6ff2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T16:29:22.891468Z",
     "iopub.status.busy": "2022-02-10T16:29:22.890600Z",
     "iopub.status.idle": "2022-02-10T16:31:34.618696Z",
     "shell.execute_reply": "2022-02-10T16:31:34.618190Z"
    },
    "papermill": {
     "duration": 181.019256,
     "end_time": "2022-02-10T16:31:34.618842",
     "exception": false,
     "start_time": "2022-02-10T16:28:33.599586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder ../train/result/result_supervised_task_adaptation\r\n",
      "preprocessing method linearized_penman\r\n",
      "preprocess amr_simple_test\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|████████████████████████████████████| 2618/2618 [00:00<00:00, 95173.89it/s]\r\n",
      "total: 306 pair sent-amr\r\n",
      "preprocess b-salah-darat\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 111557.66it/s]\r\n",
      "total: 32 pair sent-amr\r\n",
      "preprocess c-gedung-roboh\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 606/606 [00:00<00:00, 119252.52it/s]\r\n",
      "total: 29 pair sent-amr\r\n",
      "preprocess d-indo-fuji\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 122405.06it/s]\r\n",
      "total: 27 pair sent-amr\r\n",
      "preprocess f-bunuh-diri\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 126665.44it/s]\r\n",
      "total: 23 pair sent-amr\r\n",
      "preprocess g-gempa-dieng\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 103936.80it/s]\r\n",
      "total: 19 pair sent-amr\r\n",
      "evaluate on data amr_simple_test\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_supervised_task_adaptation/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:18<00:00,  4.14it/s]\r\n",
      "sample:  ilham meniup balon. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya mengetik makalah. ---- saya mengetik makalah\r\n",
      "sample:  angga anak ajaib. ---- angga anak ajaib\r\n",
      "sample:  ibu saya orang dosen. ---- ibuku seorang dosen\r\n",
      "sample:  dia berenang. ---- dia sedang berenang\r\n",
      "sample:  ilham mencari buku yang hilang. ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  para ibu menyapu di halaman rumah. ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  ayahnya membajak padi di sawah. ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  andi pergi kemah di tepi pantai. ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  35.66394591669426\r\n",
      "evaluate on data b-salah-darat\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_supervised_task_adaptation/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:06<00:00,  1.15it/s]\r\n",
      "sample:  kantor sama kantor imigrasi bandara soekarno-hatta dan pt angkasa pura ii bersama otoritas bandara soekarno-hatta membahas insiden penumpang penerbangan pesawat yang tiba di jakarta dari singapura pada 10 mei hingga 161 jt. ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarno-hatta alif suadi menuturkan, kami meeting soal ini kepada tempo, sabtu malam ini, 14 mei. ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak teman natalie dan yang menggunakan pesawat lion air jt 161 berangkat dari singapura pukul 18 55. ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  sekretaris agus haryadi dari perusahaan pt angkasa pura ii mengatakan, dalam melakukan pengelolaan bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah 1 terkait dengan perisitiwa tersebut. ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  setelah pesawat parkir di remote area, kami menyiapkan bus antar penumpang yang menumpang di terminal. ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  edward, ujarnya, mendapat sopir yang menjemput penumpang di padang dengan bus. ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak terkait dan melakukan investigasi. ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  ujarnya, kemenhub memberikan sanksi kepada pihak-pihak yang lalai. ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  zara, seperti dikutip dari cerita temannya, mengatakan pesawat tidak mendarat di terminal ii. ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  setelah pesawat parkir di remote area, kami menyiapkan bus antar penumpang yang menumpang di terminal. ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  31.69603505461242\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_supervised_task_adaptation/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:06<00:00,  1.23it/s]\r\n",
      "sample:  pengalihan kendaraan yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dilakukan dengan menggunakan flyover di atas lampu indomobil merah. ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kegagalan dan kesalahan tes tanah mengakibatkan struktur gedung miring terjadi di menara di jalan saidah, jakarta timur, di mt haryono. ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  kapolres ayi supardan akbp di tangerang selatan membenarkan peristiwa di bintaro, yakni robohnya gedung, menarik hati banyak masyarakat. ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  panin memutuskan melanjutkan pembangunan gedung tersebut karena dinyatakan tidak lulus uji kelayakan. ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung yang disebutkan itu dinyatakan tidak lulus uji sehingga bangunan yang memutuskan tidak melanjutkan pembangunan gedung itu dinyatakan tidak layak. ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kasat akp samian dari polres tangerang selatan kepada reskrim mengatakan, untuk mengetahui penyebab runtuhnya gedung tersebut, pihaknya melakukan penyelidikan pada saat ini. ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  masih kami dalami peristiwa yang disebutnya hingga saat ini. ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  disebutkan, tidak semua kejadian menimbulkan korban. ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  terjadi kerugian material. ---- terjadi kerugian material.\r\n",
      "sample:  pembongkaran di sektor 7 bintaro, tangerang selatan (7), gedung tersebut diduga menyalahi prosedur. ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  33.216995392714864\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_supervised_task_adaptation/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.09s/it]\r\n",
      "sample:  indosat bersama fujitsu indonesia di ooredoo menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra bisnis yang telah dibangun dengan menghadirkan solusi smart dan internet of things (mobility). ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  indosat bersama fujitsu indonesia di ooredoo menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra bisnis yang telah dibangun dengan menghadirkan solusi smart dan internet of things (mobility). ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  smart mobility dan iot mengalami perkembangan yang cukup signifikan hingga mengubah cara perusahaan di dalam mengelola asetnya. ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  indosat bersama fujitsu indonesia di ooredoo menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra bisnis yang telah dibangun dengan menghadirkan solusi smart dan internet of things (mobility). ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kerja sama ini memfokuskan pada sektor transportasi dan transportasi, otomotif, dan otomotif secara luas untuk memenuhi kebutuhan pelanggan korporasi di indonesia, dan sebagainya. ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president achmad sofwan, director fujitsu indonesia, menuturkan, dengan menyediakan layanan yang memproses data real-time dan memanfaatkan layanan aplikasi untuk mobilitas, tersedia cara untuk mengoptimalkan strategi perusahaan. ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  kerja sama ini memfokuskan pada sektor transportasi dan transportasi, otomotif, dan otomotif secara luas untuk memenuhi kebutuhan pelanggan korporasi di indonesia, dan sebagainya. ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  indosat ooredoo bekerja sama menghadirkan solusi smart mobility, internet of things, dan internet bersama dengan fujitsu di indonesia. ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president achmad sofwan, director fujitsu, mengungkapkan bahwa dengan menyediakan layanan memproses data real-time dan memanfaatkan layanan aplikasi, mobilitas itu disediakan sebagai cara untuk mengoptimalkan strategi perusahaan. ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  kerja sama ini memfokuskan pada sektor transportasi dan transportasi, otomotif, dan otomotif secara luas untuk memenuhi kebutuhan pelanggan korporasi di indonesia, dan sebagainya. ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  23.932033450108737\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_supervised_task_adaptation/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:06<00:00,  1.07s/it]\r\n",
      "sample:  ciptadi melompat ke megatron di jembatan yang ketinggiannya diperkirakan mencapai 1,5 meter itu untuk menyeberang orang. ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang disebutkan itu terungkap ketika kasubag reny marthaliana dari humas kompol didampingi kabag dede rojudin dari kompol yang ops di polrestabes bandung bertemu jumat di mapolrestabes bandung. ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  rezha, warga kota bandung, kepada noviana mengatakan, meski telah membubarkan salat di masjid raya bandung, jumat, ia dan jemaat lainnya memperhatikan megatron di depan kantor di bandung, sebuah pos besar di seberang jembatan penyeberangan orang. ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  petugas linmas dan berapa satpam berdiri di atas jpo. ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di alun-alun bandung, belum lama ini. ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  beberapa orang pria tua menjatuhkan diri ke jpo di alun-alun kota bandung. ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  sudirman, yang menjadi saksi mata pembunuhan di alun-alun bandung, anggota linmas kota bandung dari satpol pp, mengatakan, pria yang disebutnya diperkirakan berusia sekitar 30 tahun itu. ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  lelaki orang itu menjatuhkan diri dengan megatron di depan bandung pos giro besar bandung di jembatan penyeberangan, jumat. ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  lelaki setengah baya itu mengakhiri hidupnya di tengah padatnya lalu lintas di jalan asia afrika itu. ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  pria misterius yang bunuh diri di jembatan yang berjalan di bandung, jawa barat, asia afrika, di seberang orang membuat pesan untuk belum terjun nekat. ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  23.90025422174565\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "Running on the GPU\r\n",
      "PreTrainedTokenizerFast(name_or_path='../train/result/result_supervised_task_adaptation/tokenizer', vocab_size=32100, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "T5Config {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5ForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"num_decoder_layers\": 12,\r\n",
      "  \"num_heads\": 12,\r\n",
      "  \"num_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"tie_word_embeddings\": false,\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32109\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:04<00:00,  1.12it/s]\r\n",
      "sample:  stasiun banjarnegara, jawa tengah, badan meteorologi klimatologi geofisika (badan geofisika) menyatakan, gempa berkekuatan 4 8 skala richter mengguncang dataran tinggi dieng pada pukul 19 00 wib. ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  pusat gempa diperkirakan berada di desa di kecamatan di kabupaten batang, yakni desa tanji gugur. ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  pvmbg merekomendasikan agar sementara jalan yang berdekatan dengan kawah di desa sumberejo ditutup secara menyeluruh. ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala surono di pusat vulkanologi mitigasi bencana geologi mengatakan, sebanyak 86 kali rekaman gempa yang terjadi pada pukul 19: 00 wib di dataran tinggi dieng. ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  penutupan dilakukan setelah tim yang mengukur konsentrasi gas di lapangan menyatakan tidak ada gas berbahaya. ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  pvmbg merekomendasikan agar sementara jalan yang berdekatan dengan kawah di desa sumberejo ditutup secara menyeluruh. ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  petugas pusat vulkanologi mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi. ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  data yang dikeluarkan pusat vulkanologi mitigasi bencana geologi menyebutkan, rasa dan telah direkam gempa sebanyak 86 kali mulai gempa pada pukul 19: 00 wib. ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  para pengungsi di kabupaten banjarnegara meninggalkan pengungsi sejak sabtu pagi. ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  stasiun banjarnegara, badan meteorologi, klimatologi, geofisika, menyatakan bahwa gempa berkekuatan 4 8 skala richter mengguncang dataran tinggi dieng pada pukul 19 00 wib. ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  32.73063725526824\r\n",
      "mv: cannot move 'result/linearized_penman' to a subdirectory of itself, 'result/linearized_penman/linearized_penman'\r\n"
     ]
    }
   ],
   "source": [
    "!./evaluate_indoT5_to_all.sh ../train/result/result_supervised_task_adaptation linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c89910d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T16:33:14.595064Z",
     "iopub.status.busy": "2022-02-10T16:33:14.594292Z",
     "iopub.status.idle": "2022-02-10T16:33:15.327397Z",
     "shell.execute_reply": "2022-02-10T16:33:15.326740Z"
    },
    "papermill": {
     "duration": 50.653731,
     "end_time": "2022-02-10T16:33:15.327541",
     "exception": false,
     "start_time": "2022-02-10T16:32:24.673810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_generations.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_generations.txt (deflated 61%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_generations.txt (deflated 86%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result.zip result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21325.906138,
   "end_time": "2022-02-10T16:34:05.985855",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-10T10:38:40.079717",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
