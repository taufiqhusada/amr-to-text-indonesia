{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b6aff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T10:31:08.003156Z",
     "iopub.status.busy": "2022-02-10T10:31:08.002476Z",
     "iopub.status.idle": "2022-02-10T10:31:41.976816Z",
     "shell.execute_reply": "2022-02-10T10:31:41.975551Z"
    },
    "papermill": {
     "duration": 33.999451,
     "end_time": "2022-02-10T10:31:41.977085",
     "exception": false,
     "start_time": "2022-02-10T10:31:07.977634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.12.5)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\r\n",
      "Collecting indobenchmark-toolkit==0.0.4\r\n",
      "  Downloading indobenchmark_toolkit-0.0.4-py3-none-any.whl (8.0 kB)\r\n",
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\r\n",
      "     |████████████████████████████████| 90 kB 724 kB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from indobenchmark-toolkit==0.0.4) (1.9.1)\r\n",
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "     |████████████████████████████████| 1.2 MB 2.8 MB/s            \r\n",
      "\u001b[?25hCollecting datasets==1.4.1\r\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\r\n",
      "     |████████████████████████████████| 186 kB 39.9 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.19.5)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.11.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (4.8.2)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.3.4)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.70.12.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.2)\r\n",
      "Collecting huggingface-hub==0.0.2\r\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (6.0.0)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.3.4)\r\n",
      "Collecting tqdm<4.50.0,>=4.27\r\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\r\n",
      "     |████████████████████████████████| 69 kB 5.1 MB/s             \r\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.2->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.3.2)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 40.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 33.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 31.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 36.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 38.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\r\n",
      "     |████████████████████████████████| 3.3 MB 34.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 43.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 37.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 39.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.1-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 42.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 38.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 34.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 39.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.1-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 37.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 36.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 35.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 39.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 33.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 34.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 30.0 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\r\n",
      "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 39.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 39.2 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 31.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 35.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 25.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 36.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\r\n",
      "     |████████████████████████████████| 2.2 MB 36.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\r\n",
      "     |████████████████████████████████| 2.3 MB 31.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\r\n",
      "     |████████████████████████████████| 2.1 MB 38.5 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.4)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.3.2)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.8.9)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (4.0.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.26.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.10)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7.1->indobenchmark-toolkit==0.0.4) (3.10.0.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.6.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (3.0.6)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.8.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.3)\r\n",
      "Installing collected packages: tqdm, huggingface-hub, transformers, sentencepiece, datasets, sacrebleu, indobenchmark-toolkit\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.62.3\r\n",
      "    Uninstalling tqdm-4.62.3:\r\n",
      "      Successfully uninstalled tqdm-4.62.3\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.1.2\r\n",
      "    Uninstalling huggingface-hub-0.1.2:\r\n",
      "      Successfully uninstalled huggingface-hub-0.1.2\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.12.5\r\n",
      "    Uninstalling transformers-4.12.5:\r\n",
      "      Successfully uninstalled transformers-4.12.5\r\n",
      "  Attempting uninstall: sentencepiece\r\n",
      "    Found existing installation: sentencepiece 0.1.96\r\n",
      "    Uninstalling sentencepiece-0.1.96:\r\n",
      "      Successfully uninstalled sentencepiece-0.1.96\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 1.16.1\r\n",
      "    Uninstalling datasets-1.16.1:\r\n",
      "      Successfully uninstalled datasets-1.16.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "beatrix-jupyterlab 3.1.4 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "cached-path 0.3.2 requires huggingface-hub<0.2.0,>=0.0.12, but you have huggingface-hub 0.0.2 which is incompatible.\r\n",
      "cached-path 0.3.2 requires tqdm<4.63,>=4.62, but you have tqdm 4.49.0 which is incompatible.\r\n",
      "allennlp 2.8.0 requires huggingface-hub>=0.0.16, but you have huggingface-hub 0.0.2 which is incompatible.\u001b[0m\r\n",
      "Successfully installed datasets-1.4.1 huggingface-hub-0.0.2 indobenchmark-toolkit-0.0.4 sacrebleu-2.0.0 sentencepiece-0.1.95 tqdm-4.49.0 transformers-4.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece indobenchmark-toolkit==0.0.4 sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4711345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T10:31:42.122713Z",
     "iopub.status.busy": "2022-02-10T10:31:42.121758Z",
     "iopub.status.idle": "2022-02-10T10:31:44.693803Z",
     "shell.execute_reply": "2022-02-10T10:31:44.692846Z"
    },
    "papermill": {
     "duration": 2.64709,
     "end_time": "2022-02-10T10:31:44.693960",
     "exception": false,
     "start_time": "2022-02-10T10:31:42.046870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-to-text-indonesia'...\r\n",
      "remote: Enumerating objects: 1422, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (1422/1422), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (930/930), done.\u001b[K\r\n",
      "remote: Total 1422 (delta 562), reused 1169 (delta 314), pack-reused 0\u001b[K\r\n",
      "Receiving objects: 100% (1422/1422), 1.71 MiB | 3.94 MiB/s, done.\r\n",
      "Resolving deltas: 100% (562/562), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-to-text-indonesia.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "febf280d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T10:31:44.857234Z",
     "iopub.status.busy": "2022-02-10T10:31:44.851017Z",
     "iopub.status.idle": "2022-02-10T10:31:57.378991Z",
     "shell.execute_reply": "2022-02-10T10:31:57.378466Z"
    },
    "papermill": {
     "duration": 12.609717,
     "end_time": "2022-02-10T10:31:57.379185",
     "exception": false,
     "start_time": "2022-02-10T10:31:44.769468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-indo-dataset'...\r\n",
      "remote: Enumerating objects: 57, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (57/57), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\r\n",
      "remote: Total 57 (delta 12), reused 53 (delta 11), pack-reused 0\u001b[K\r\n",
      "Unpacking objects: 100% (57/57), 55.48 MiB | 7.24 MiB/s, done.\r\n",
      "Updating files: 100% (50/50), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-indo-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0229303d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T10:31:57.555023Z",
     "iopub.status.busy": "2022-02-10T10:31:57.552102Z",
     "iopub.status.idle": "2022-02-10T10:31:57.558866Z",
     "shell.execute_reply": "2022-02-10T10:31:57.559374Z"
    },
    "papermill": {
     "duration": 0.095483,
     "end_time": "2022-02-10T10:31:57.559544",
     "exception": false,
     "start_time": "2022-02-10T10:31:57.464061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/train\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/train\n",
    "# %cd /kaggle/working/amr-indo-dataset\n",
    "# !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b4c500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T10:31:57.737599Z",
     "iopub.status.busy": "2022-02-10T10:31:57.736749Z",
     "iopub.status.idle": "2022-02-10T14:30:25.654367Z",
     "shell.execute_reply": "2022-02-10T14:30:25.654857Z"
    },
    "papermill": {
     "duration": 14308.011146,
     "end_time": "2022-02-10T14:30:25.655042",
     "exception": false,
     "start_time": "2022-02-10T10:31:57.643896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing method linearized_penman\r\n",
      "silver data folder ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/preprocessed_silver_data/train.amr.txt', result_sent_path='data/preprocessed_silver_data/train.sent.txt', source_file_path=None, source_folder_path='../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news')\r\n",
      "100%|███████████████████████████████| 263263/263263 [00:02<00:00, 131605.55it/s]\r\n",
      "100%|████████████████████████████████| 264253/264253 [00:02<00:00, 88784.83it/s]\r\n",
      "100%|███████████████████████████████| 270251/270251 [00:02<00:00, 126012.91it/s]\r\n",
      "100%|███████████████████████████████| 264330/264330 [00:01<00:00, 134733.01it/s]\r\n",
      "100%|███████████████████████████████| 263508/263508 [00:01<00:00, 132250.17it/s]\r\n",
      "100%|███████████████████████████████| 268572/268572 [00:02<00:00, 126276.24it/s]\r\n",
      "100%|███████████████████████████████| 268262/268262 [00:02<00:00, 113579.93it/s]\r\n",
      "100%|███████████████████████████████| 269258/269258 [00:02<00:00, 129796.31it/s]\r\n",
      "100%|███████████████████████████████| 266966/266966 [00:02<00:00, 131534.54it/s]\r\n",
      "100%|███████████████████████████████| 265325/265325 [00:02<00:00, 131111.50it/s]\r\n",
      "total: 154892 pair sent-amr\r\n",
      "Running on the GPU\r\n",
      "Downloading: 100%|███████████████████████████| 932k/932k [00:00<00:00, 1.97MB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 315/315 [00:00<00:00, 213kB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 339/339 [00:00<00:00, 207kB/s]\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Downloading: 100%|█████████████████████████| 1.71k/1.71k [00:00<00:00, 1.24MB/s]\r\n",
      "Downloading: 100%|███████████████████████████| 526M/526M [00:38<00:00, 13.6MB/s]\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  154892\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  38723\r\n",
      "(Epoch 1) TRAIN LOSS:1.0734 LR:0.00003000: 100%|█| 38723/38723 [1:13:55<00:00,  \r\n",
      "(Epoch 1) DEV LOSS:3.4926 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 32.90it/s]\r\n",
      "bleu score on dev:  16.391481034926517\r\n",
      "(Epoch 2) TRAIN LOSS:0.7789 LR:0.00003000: 100%|█| 38723/38723 [1:19:14<00:00,  \r\n",
      "(Epoch 2) DEV LOSS:3.4930 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 30.18it/s]\r\n",
      "bleu score on dev:  8.841476712562002\r\n",
      "(Epoch 3) TRAIN LOSS:0.6631 LR:0.00003000: 100%|█| 38723/38723 [1:23:28<00:00,  \r\n",
      "(Epoch 3) DEV LOSS:3.2993 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 32.73it/s]\r\n",
      "bleu score on dev:  12.29421933481922\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:15<00:00,  4.83it/s]\r\n",
      "sample:  ilham meniupkan balon. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya ketik makalahnya. ---- saya mengetik makalah\r\n",
      "sample:  oni anak ajaib. ---- angga anak ajaib\r\n",
      "sample:  ibunya orang dosen. ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  29.214781518322063\r\n",
      "tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\r\n",
      "           384,   384,   384,     2, 40002]])\r\n",
      "saya ketik makalahnya.\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x train_indoBART_on_silver_data.sh\n",
    "!./train_indoBART_on_silver_data.sh linearized_penman ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a78c231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T14:32:53.497571Z",
     "iopub.status.busy": "2022-02-10T14:32:53.496467Z",
     "iopub.status.idle": "2022-02-10T14:32:53.500427Z",
     "shell.execute_reply": "2022-02-10T14:32:53.501029Z"
    },
    "papermill": {
     "duration": 74.057451,
     "end_time": "2022-02-10T14:32:53.501234",
     "exception": false,
     "start_time": "2022-02-10T14:31:39.443783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/evaluate\n"
     ]
    }
   ],
   "source": [
    "%cd ../evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf468e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T14:35:21.969592Z",
     "iopub.status.busy": "2022-02-10T14:35:21.968578Z",
     "iopub.status.idle": "2022-02-10T14:35:22.731060Z",
     "shell.execute_reply": "2022-02-10T14:35:22.731933Z"
    },
    "papermill": {
     "duration": 74.712796,
     "end_time": "2022-02-10T14:35:22.732180",
     "exception": false,
     "start_time": "2022-02-10T14:34:08.019384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x evaluate_indoBART_to_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac37aa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T14:37:52.012095Z",
     "iopub.status.busy": "2022-02-10T14:37:52.011178Z",
     "iopub.status.idle": "2022-02-10T14:40:13.080469Z",
     "shell.execute_reply": "2022-02-10T14:40:13.079800Z"
    },
    "papermill": {
     "duration": 215.397015,
     "end_time": "2022-02-10T14:40:13.080662",
     "exception": false,
     "start_time": "2022-02-10T14:36:37.683647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder ../train/result/result_supervised_task_adaptation\r\n",
      "preprocessing method linearized_penman\r\n",
      "preprocess amr_simple_test\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 153797.61it/s]\r\n",
      "total: 306 pair sent-amr\r\n",
      "preprocess b-salah-darat\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 110106.38it/s]\r\n",
      "total: 32 pair sent-amr\r\n",
      "preprocess c-gedung-roboh\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 606/606 [00:00<00:00, 114302.66it/s]\r\n",
      "total: 29 pair sent-amr\r\n",
      "preprocess d-indo-fuji\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 111906.19it/s]\r\n",
      "total: 27 pair sent-amr\r\n",
      "preprocess f-bunuh-diri\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 116040.10it/s]\r\n",
      "total: 23 pair sent-amr\r\n",
      "preprocess g-gempa-dieng\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 106106.67it/s]\r\n",
      "total: 19 pair sent-amr\r\n",
      "evaluate on data amr_simple_test\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:15<00:00,  4.84it/s]\r\n",
      "sample:  ilham meniupkan balon. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya ketik makalahnya. ---- saya mengetik makalah\r\n",
      "sample:  oni anak ajaib. ---- angga anak ajaib\r\n",
      "sample:  ibunya orang dosen. ---- ibuku seorang dosen\r\n",
      "sample:  dia berenang. ---- dia sedang berenang\r\n",
      "sample:  ilham mencari buku yang hilang. ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibunya menyapu di halaman rumahnya. ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  ayahnya pembajak padi di sawah. ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  andi pergi ke kemah di tepi pantai. ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  29.214781518322063\r\n",
      "evaluate on data b-salah-darat\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.37it/s]\r\n",
      "sample:  kantor kantor kantor sama dan pt angkasa pura ii otoritas bandara soekarno-hatta di imigrasi bandara soekarnoohatta membahas insiden para penumpang dalam penerbangan pesawat yang tiba di jakarta dari singapura, sekitar 10 mei di lion air, jt (16/1). ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarno-hatta alif suadi, ujar kami meeting soal itu kepada tempo pada sabtu malam (1/4) ini, 14 mei. ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak temannya, nat jitu, dan yang menggunakan pesawat lion air jt 161, berangkat sekitar pukul 18:5 ke singapura. ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  sekretaris agus haryadi, perusahaan pt angkasa pura ii, mengatakan, pelaku yang mengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah 1, yang terkait dengan 212/2itiwa. ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  setelah pesawat parkir di remote area kami menyiapkan bus antar penumpang di terminal. ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  edward, ujar seorang sopir yang menjemput penumpang di padang dengan bus. ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil semua pihak yang terkait dan melakukan investigasi. ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  ujarnya, setelah memberi sanksi kepadanya, pihak-pihak yang lalai. ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  bekas-setelah mengutip cerita temannya mengatakan, pesawat itu tidak mendarat di terminal ii. ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  setelah pesawat parkir di remote area kami menyiapkan bus antar penumpang di terminal. ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  25.76514380630465\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:04<00:00,  1.89it/s]\r\n",
      "sample:  dengan pengalihan itu, kendaraan melaju dari arah mal bintaro ke bintaro dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu indomobil merah. ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kegagalan dan kesalahan yang disalahkan dalam tes tanah mengakibatkan struktur gedung miring itu terjadi di menara di jalan mt haryono, jakarta timur, di saidah. ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  kapolres ayi supardan, akbp di tangerang selatan, membenarkan peristiwa di bintaro yang merobohkan gedung itu karena menarik hati banyak masyarakat. ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  panin memutuskan untuk melanjutkan pembangunan gedung itu dikarenakan yang dinyatakan tidak lulus uji kelayakan. ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  sejumlah gedung yang disebutkan dinyatakan tidak lulus ujian menyatakan bahwa bangunan yang diputuskan tidak berkelanjutan sehingga gedung itu putus. ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kasat akp samian dari polres tangerang selatan ke polres tangerang mengatakan, pihaknya berencana untuk mengetahui penyebab runtuhnya gedung tersebut pada saat ini. ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  masih kami mendalami peristiwa yang disebut itu hingga ini. ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  disebutkan, tidak jadi menimbulkan korban. ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  terjadi kerugian material. ---- terjadi kerugian material.\r\n",
      "sample:  sementara itu, di sektor bintaro, tangerang selatan, 7/7, gedung itu diduga menyalahi prosedur. ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  25.641555888575077\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:04<00:00,  1.41it/s]\r\n",
      "sample:  sementara itu, indosat yang sama dengan agus dwikarna, indonesia, menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra bangun yang telah pelanggan bisnis yang mendalam dalam hadir dalam solusi kemacetan dan internet (international mobility). ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  sementara itu, indosat yang sama dengan agus dwikarna, indonesia, menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra bangun yang telah pelanggan bisnis yang mendalam dalam hadir dalam solusi kemacetan dan internet (international mobility). ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  hebatnya, mobility dan kekayaan alam alam alam mengalami perkembangan yang cukup signifikan, seperti yang disampaikannya dengan mengubah cara para pengusaha pedalaman mengelola asetnya. ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  sementara itu, indosat yang sama dengan agus dwikarna, indonesia, menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra bangun yang telah pelanggan bisnis yang mendalam dalam hadir dalam solusi kemacetan dan internet (international mobility). ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kerja sama itu terfokus pada sektor transportasi dan transportasi, sektor otomotif yang luas dan memadai untuk memenuhi kebutuhan pelanggan korporasi di indonesia, dan sektor industri. ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president achmad sofwan, director fujita indonesia, menuturkan, untuk menyediakan layanan mobilitas dengan cara mengoptimalkan strategi perusahaan, memproses data real madrid dan menyediakan aplikasi. ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  kerja sama itu terfokus pada sektor transportasi dan transportasi, sektor otomotif yang luas dan memadai untuk memenuhi kebutuhan pelanggan korporasi di indonesia, dan sektor industri. ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  selain itu, indosat bekerja sama untuk menghadirkan solusi yang berbeda, yaitu semarang, jateng, mobility, dan jepang, di indonesia. ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president achmad sofwan (direktur fujita) mengungkapkan, layanan yang memproses data real-waktu dan menyediakan mobilitas itu dengan cara mengoptimalkan strategi perusahaan. ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  kerja sama itu terfokus pada sektor transportasi dan transportasi, sektor otomotif yang luas dan memadai untuk memenuhi kebutuhan pelanggan korporasi di indonesia, dan sektor industri. ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  14.22025351059366\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:03<00:00,  1.51it/s]\r\n",
      "sample:  bola-karangan melompat ke jembatan yang ketinggiannya diperkirakan mencapai 15 meter, sebrang orang. ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang disebutkan itu diungkapkan kasubag reny marthaliana, humas kompol kabag dede rukoudin, kompol ops di polron bandung, jumat, usai ditemui di mapolsek, bandung. ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  rezha, warga kota bandung, kepada noviana, mengatakan, ia dan jemaat lainnya bubar untuk salat di masjid raya bandung, jumat di depan jembatan bandung di seberang pos orang di jembatan penyeberangan bandung. ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  petugas linmas dan satpam berapa pun berdiri di atas jpo itu. ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri itu terjadi di alun- alun bandung, belum lama ini. ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  orang tua pria tua itu jatuh diri ke jpo di alun- alun di kota bandung. ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  sudirman, yang menjadi saksi mata pembunuh di alun alun bandung, mengatakan, dirinya anggota linmas kota bandung dari satpol pp, diperkirakan seorang pria dengan sebutan itu sekitar 30 tahun. ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  lelaki orang itu jatuh diri di bandung, di depan pos giro terbesar bandung di jembatan penyeberangan pada jumat. ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  lelaki setengah baya itu mengakhiri hidup di tengah kepadatan lalu lintas di jalan asia afrika yang dilaluinya. ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  seorang pria misterius yang terbunuh di jembatan yang berjalan di bandung, jawa barat, di asia afrika, di seberang orang membuat dirinya belum terjun nekat. ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  22.170776898154816\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_supervised_task_adaptation/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54it/s]\r\n",
      "sample:  stasiun di banjarnegara, jawa tengah, badan meteorologi dan geofisika menyatakan, gempa berkekuatan 4, 8 pada skala richter mengguncang dataran tinggi dieng pada pukul 19: 00 wib. ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  pusat gempa diperkirakan berada di desa tanji, kecamatan gugur, kabupaten batang, di tanji. ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  museum-mmbg merekomendasikan agar sementara jalan yang berdekatan dengan kawah di desa sumberejo itu dipertimbangkan seluruhnya tutup. ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala surono dari pusat vulkanologi mitigasi bencana geologi mengatakan, sebanyak 86 kali rekaman gempa yang terjadi sekitar pukul 19: 00 wib di dataran tinggi dieng. ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  penutupan dilakukan, seperti yang disampaikan tim yang mengukur konsentrasi gas di lapangan menyatakan tidak ada gas berbahaya. ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  museum-mmbg merekomendasikan agar sementara jalan yang berdekatan dengan kawah di desa sumberejo itu dipertimbangkan seluruhnya tutup. ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  menurut petugas dari pusat vulkanologi, mitigasi bencana geologi menerjunkan beberapa pengamat untuk memantau aktivitas di kawah dengan mempertimbangkan dan memberikan informasi. ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  data yang dikeluarkan pusat vulkanologi mitigasi bencana geologi menyebutkan, dan telah direkam gempa sebanyak 8,6 kali, ketika gempa mulai dirasakan pada pukul 19: 00 wib. ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi di kabupaten banjarnegara meninggalkan pengungsian pada sabtu pagi. ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  sementara itu, di banjarnegara, badan meteorologi ( geofisika) menyatakan, gempa berkekuatan 4, 8 skala richter mengguncang dataran tinggi dieng pada pukul 19: 00 wib. ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  22.775475375804433\r\n",
      "mv: cannot move 'result/linearized_penman' to a subdirectory of itself, 'result/linearized_penman/linearized_penman'\r\n"
     ]
    }
   ],
   "source": [
    "!./evaluate_indoBART_to_all.sh ../train/result/result_supervised_task_adaptation linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0e7a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T14:42:43.058193Z",
     "iopub.status.busy": "2022-02-10T14:42:43.057208Z",
     "iopub.status.idle": "2022-02-10T14:42:43.825661Z",
     "shell.execute_reply": "2022-02-10T14:42:43.824994Z"
    },
    "papermill": {
     "duration": 75.797002,
     "end_time": "2022-02-10T14:42:43.825815",
     "exception": false,
     "start_time": "2022-02-10T14:41:28.028813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_generations.txt (deflated 83%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result.zip result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15180.298032,
   "end_time": "2022-02-10T14:43:58.603507",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-10T10:30:58.305475",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
