{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724896f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:07:25.210734Z",
     "iopub.status.busy": "2022-05-24T00:07:25.209254Z",
     "iopub.status.idle": "2022-05-24T00:07:56.477814Z",
     "shell.execute_reply": "2022-05-24T00:07:56.478322Z",
     "shell.execute_reply.started": "2022-03-14T06:35:28.661684Z"
    },
    "papermill": {
     "duration": 31.302561,
     "end_time": "2022-05-24T00:07:56.478583",
     "exception": false,
     "start_time": "2022-05-24T00:07:25.176022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.15.0)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\r\n",
      "Collecting indobenchmark-toolkit==0.0.4\r\n",
      "  Downloading indobenchmark_toolkit-0.0.4-py3-none-any.whl (8.0 kB)\r\n",
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\r\n",
      "     |████████████████████████████████| 92 kB 964 kB/s            \r\n",
      "\u001b[?25hCollecting datasets==1.4.1\r\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\r\n",
      "     |████████████████████████████████| 186 kB 4.0 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from indobenchmark-toolkit==0.0.4) (1.9.1)\r\n",
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "     |████████████████████████████████| 1.2 MB 10.8 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.26.0)\r\n",
      "Collecting tqdm<4.50.0,>=4.27\r\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\r\n",
      "     |████████████████████████████████| 69 kB 6.8 MB/s             \r\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.70.12.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2022.2.0)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (0.3.4)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (6.0.1)\r\n",
      "Collecting xxhash\r\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\r\n",
      "     |████████████████████████████████| 212 kB 56.7 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (4.11.2)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.3.5)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.20.3)\r\n",
      "Collecting huggingface-hub==0.0.2\r\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.2->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.4.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\r\n",
      "     |████████████████████████████████| 4.2 MB 63.0 MB/s            \r\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\r\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\r\n",
      "     |████████████████████████████████| 6.6 MB 47.5 MB/s            \r\n",
      "\u001b[?25hCollecting transformers\r\n",
      "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\r\n",
      "     |████████████████████████████████| 4.2 MB 53.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.19.0-py3-none-any.whl (4.2 MB)\r\n",
      "     |████████████████████████████████| 4.2 MB 52.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\r\n",
      "     |████████████████████████████████| 4.0 MB 45.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\r\n",
      "     |████████████████████████████████| 3.8 MB 43.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 46.1 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\r\n",
      "  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 46.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\r\n",
      "     |████████████████████████████████| 3.5 MB 42.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 43.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\r\n",
      "     |████████████████████████████████| 3.3 MB 54.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 55.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 45.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 50.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 36.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.1-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 23.3 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\r\n",
      "     |████████████████████████████████| 3.1 MB 45.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 43.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 39.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.1-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 44.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\r\n",
      "     |████████████████████████████████| 2.9 MB 45.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 51.0 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 12.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 44.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\r\n",
      "     |████████████████████████████████| 2.8 MB 46.5 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 44.7 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 50.9 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 44.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 41.9 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 47.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 49.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 39.8 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\r\n",
      "     |████████████████████████████████| 2.2 MB 49.4 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\r\n",
      "     |████████████████████████████████| 2.3 MB 40.6 MB/s            \r\n",
      "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\r\n",
      "     |████████████████████████████████| 2.1 MB 43.9 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.47)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.4.0)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.4)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.8.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (1.26.7)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.0.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.10.8)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7.1->indobenchmark-toolkit==0.0.4) (4.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (3.6.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.4.1->indobenchmark-toolkit==0.0.4) (2021.3)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, transformers, sentencepiece, datasets, sacrebleu, indobenchmark-toolkit\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.62.3\r\n",
      "    Uninstalling tqdm-4.62.3:\r\n",
      "      Successfully uninstalled tqdm-4.62.3\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.2.1\r\n",
      "    Uninstalling huggingface-hub-0.2.1:\r\n",
      "      Successfully uninstalled huggingface-hub-0.2.1\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.15.0\r\n",
      "    Uninstalling transformers-4.15.0:\r\n",
      "      Successfully uninstalled transformers-4.15.0\r\n",
      "  Attempting uninstall: sentencepiece\r\n",
      "    Found existing installation: sentencepiece 0.1.96\r\n",
      "    Uninstalling sentencepiece-0.1.96:\r\n",
      "      Successfully uninstalled sentencepiece-0.1.96\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "beatrix-jupyterlab 3.1.6 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "spacy 3.2.2 requires typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.1.1 which is incompatible.\r\n",
      "featuretools 1.6.0 requires numpy>=1.21.0, but you have numpy 1.20.3 which is incompatible.\r\n",
      "cached-path 1.0.2 requires huggingface-hub<0.3.0,>=0.0.12, but you have huggingface-hub 0.0.2 which is incompatible.\r\n",
      "cached-path 1.0.2 requires tqdm<4.63,>=4.62, but you have tqdm 4.49.0 which is incompatible.\r\n",
      "allennlp 2.9.0 requires huggingface-hub>=0.0.16, but you have huggingface-hub 0.0.2 which is incompatible.\r\n",
      "allennlp 2.9.0 requires tqdm>=4.62, but you have tqdm 4.49.0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed datasets-1.4.1 huggingface-hub-0.0.2 indobenchmark-toolkit-0.0.4 sacrebleu-2.1.0 sentencepiece-0.1.95 tqdm-4.49.0 transformers-4.5.1 xxhash-3.0.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece indobenchmark-toolkit==0.0.4 sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6812a84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:07:56.664461Z",
     "iopub.status.busy": "2022-05-24T00:07:56.663692Z",
     "iopub.status.idle": "2022-05-24T00:08:00.562905Z",
     "shell.execute_reply": "2022-05-24T00:08:00.562068Z",
     "shell.execute_reply.started": "2022-03-14T06:36:04.472712Z"
    },
    "papermill": {
     "duration": 3.991839,
     "end_time": "2022-05-24T00:08:00.563050",
     "exception": false,
     "start_time": "2022-05-24T00:07:56.571211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-to-text-indonesia'...\r\n",
      "remote: Enumerating objects: 2123, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (268/268), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (199/199), done.\u001b[K\r\n",
      "remote: Total 2123 (delta 74), reused 153 (delta 37), pack-reused 1855\u001b[K\r\n",
      "Receiving objects: 100% (2123/2123), 19.11 MiB | 15.55 MiB/s, done.\r\n",
      "Resolving deltas: 100% (840/840), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-to-text-indonesia.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117d7a2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:08:00.760259Z",
     "iopub.status.busy": "2022-05-24T00:08:00.759508Z",
     "iopub.status.idle": "2022-05-24T00:08:12.633341Z",
     "shell.execute_reply": "2022-05-24T00:08:12.632761Z",
     "shell.execute_reply.started": "2022-03-14T06:36:07.592912Z"
    },
    "papermill": {
     "duration": 11.974406,
     "end_time": "2022-05-24T00:08:12.633501",
     "exception": false,
     "start_time": "2022-05-24T00:08:00.659095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amr-indo-dataset'...\r\n",
      "remote: Enumerating objects: 71, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\r\n",
      "remote: Total 71 (delta 1), reused 3 (delta 0), pack-reused 66\u001b[K\r\n",
      "Unpacking objects: 100% (71/71), 60.94 MiB | 6.54 MiB/s, done.\r\n",
      "Updating files: 100% (34/34), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/taufiqhusada/amr-indo-dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204bb711",
   "metadata": {
    "papermill": {
     "duration": 0.108182,
     "end_time": "2022-05-24T00:08:12.848945",
     "exception": false,
     "start_time": "2022-05-24T00:08:12.740763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune indobart with linearized penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa9024d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:08:13.069095Z",
     "iopub.status.busy": "2022-05-24T00:08:13.068247Z",
     "iopub.status.idle": "2022-05-24T00:08:13.071561Z",
     "shell.execute_reply": "2022-05-24T00:08:13.072119Z",
     "shell.execute_reply.started": "2022-03-14T06:38:45.630235Z"
    },
    "papermill": {
     "duration": 0.116804,
     "end_time": "2022-05-24T00:08:13.072310",
     "exception": false,
     "start_time": "2022-05-24T00:08:12.955506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/train\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf14f820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:08:13.295402Z",
     "iopub.status.busy": "2022-05-24T00:08:13.289960Z",
     "iopub.status.idle": "2022-05-24T00:08:14.117746Z",
     "shell.execute_reply": "2022-05-24T00:08:14.118777Z"
    },
    "papermill": {
     "duration": 0.939359,
     "end_time": "2022-05-24T00:08:14.118945",
     "exception": false,
     "start_time": "2022-05-24T00:08:13.179586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'hyperparam_tuning_again' set up to track remote branch 'hyperparam_tuning_again' from 'origin'.\r\n",
      "Switched to a new branch 'hyperparam_tuning_again'\r\n"
     ]
    }
   ],
   "source": [
    "!git checkout hyperparam_tuning_again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8ec454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:08:14.372902Z",
     "iopub.status.busy": "2022-05-24T00:08:14.372117Z",
     "iopub.status.idle": "2022-05-24T00:08:15.029877Z",
     "shell.execute_reply": "2022-05-24T00:08:15.029374Z",
     "shell.execute_reply.started": "2022-03-14T06:38:45.864254Z"
    },
    "papermill": {
     "duration": 0.80304,
     "end_time": "2022-05-24T00:08:15.030025",
     "exception": false,
     "start_time": "2022-05-24T00:08:14.226985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuning_indoBART.sh\ttrain_indoBART_on_silver_data.sh\r\n",
      "finetuning_indoT5.sh\ttrain_indoT5.py\r\n",
      "result\t\t\ttrain_indoT5_on_silver_data.sh\r\n",
      "train_indoBART.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f1455a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:08:15.259154Z",
     "iopub.status.busy": "2022-05-24T00:08:15.258346Z",
     "iopub.status.idle": "2022-05-24T00:12:00.476782Z",
     "shell.execute_reply": "2022-05-24T00:12:00.477316Z",
     "shell.execute_reply.started": "2022-03-14T06:38:46.955663Z"
    },
    "papermill": {
     "duration": 225.338251,
     "end_time": "2022-05-24T00:12:00.477482",
     "exception": false,
     "start_time": "2022-05-24T00:08:15.139231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Downloading: 100%|███████████████████████████| 932k/932k [00:00<00:00, 1.42MB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 315/315 [00:00<00:00, 301kB/s]\r\n",
      "Downloading: 100%|██████████████████████████████| 339/339 [00:00<00:00, 306kB/s]\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Downloading: 100%|█████████████████████████| 1.71k/1.71k [00:00<00:00, 1.58MB/s]\r\n",
      "Downloading: 100%|███████████████████████████| 526M/526M [00:23<00:00, 22.4MB/s]\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3601 LR:0.00003000: 100%|█| 662/662 [00:40<00:00, 16.38it\r\n",
      "(Epoch 1) DEV LOSS:0.5929 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 54.06it/s]\r\n",
      "bleu score on dev:  57.38434026338936\r\n",
      "(Epoch 2) TRAIN LOSS:0.5418 LR:0.00003000: 100%|█| 662/662 [00:39<00:00, 16.57it\r\n",
      "(Epoch 2) DEV LOSS:0.6770 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 60.17it/s]\r\n",
      "bleu score on dev:  56.57797910100631\r\n",
      "(Epoch 3) TRAIN LOSS:0.4376 LR:0.00003000: 100%|█| 662/662 [00:39<00:00, 16.73it\r\n",
      "(Epoch 3) DEV LOSS:0.4651 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 41.05it/s]\r\n",
      "bleu score on dev:  59.79251743606556\r\n",
      "(Epoch 4) TRAIN LOSS:0.3903 LR:0.00003000: 100%|█| 662/662 [00:40<00:00, 16.54it\r\n",
      "(Epoch 4) DEV LOSS:0.5798 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 58.83it/s]\r\n",
      "bleu score on dev:  56.06612825420897\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:11<00:00,  6.63it/s]\r\n",
      "sample:  balon tersebut ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  obe sedang menulis puisi ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak yang ajaib itu bernama angga ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  32.137943149928766\r\n",
      "tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\r\n",
      "           384,   384,   384,     2, 40002]])\r\n",
      "saya sedang mengetik makalah\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99207022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:12:02.953512Z",
     "iopub.status.busy": "2022-05-24T00:12:02.952614Z",
     "iopub.status.idle": "2022-05-24T00:12:03.755139Z",
     "shell.execute_reply": "2022-05-24T00:12:03.754650Z",
     "shell.execute_reply.started": "2022-03-14T06:44:57.875847Z"
    },
    "papermill": {
     "duration": 1.953026,
     "end_time": "2022-05-24T00:12:03.755293",
     "exception": false,
     "start_time": "2022-05-24T00:12:01.802267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu_score_test.txt  loss_data.tsv  model  test_generations.txt  test_label.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86962bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:12:06.004570Z",
     "iopub.status.busy": "2022-05-24T00:12:06.003637Z",
     "iopub.status.idle": "2022-05-24T00:12:35.038729Z",
     "shell.execute_reply": "2022-05-24T00:12:35.038268Z",
     "shell.execute_reply.started": "2022-03-14T06:44:58.623221Z"
    },
    "papermill": {
     "duration": 30.153501,
     "end_time": "2022-05-24T00:12:35.038858",
     "exception": false,
     "start_time": "2022-05-24T00:12:04.885357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/test_label.txt (deflated 60%)\r\n",
      "  adding: result/loss_data.tsv (deflated 33%)\r\n",
      "  adding: result/model/ (stored 0%)\r\n",
      "  adding: result/model/config.json (deflated 63%)\r\n",
      "  adding: result/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/.gitignore (stored 0%)\r\n",
      "  adding: result/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/test_generations.txt (deflated 64%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_without_sta.zip result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65991b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:12:37.211808Z",
     "iopub.status.busy": "2022-05-24T00:12:37.211251Z",
     "iopub.status.idle": "2022-05-24T00:12:37.215409Z",
     "shell.execute_reply": "2022-05-24T00:12:37.214932Z",
     "shell.execute_reply.started": "2022-03-14T06:45:31.282452Z"
    },
    "papermill": {
     "duration": 1.102947,
     "end_time": "2022-05-24T00:12:37.215535",
     "exception": false,
     "start_time": "2022-05-24T00:12:36.112588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/evaluate\n"
     ]
    }
   ],
   "source": [
    "%cd ../evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fe3f166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:12:39.434137Z",
     "iopub.status.busy": "2022-05-24T00:12:39.429067Z",
     "iopub.status.idle": "2022-05-24T00:14:30.128390Z",
     "shell.execute_reply": "2022-05-24T00:14:30.127546Z",
     "shell.execute_reply.started": "2022-03-14T06:45:31.330294Z"
    },
    "papermill": {
     "duration": 111.828052,
     "end_time": "2022-05-24T00:14:30.128531",
     "exception": false,
     "start_time": "2022-05-24T00:12:38.300479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder ../train/result\r\n",
      "preprocessing method linearized_penman\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 113075.90it/s]\r\n",
      "total: 306 pair sent-amr\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 676/676 [00:00<00:00, 77278.54it/s]\r\n",
      "total: 32 pair sent-amr\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 606/606 [00:00<00:00, 50131.12it/s]\r\n",
      "total: 29 pair sent-amr\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 745/745 [00:00<00:00, 77730.26it/s]\r\n",
      "total: 27 pair sent-amr\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 468/468 [00:00<00:00, 80682.90it/s]\r\n",
      "total: 23 pair sent-amr\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 130488.05it/s]\r\n",
      "total: 19 pair sent-amr\r\n",
      "mkdir: cannot create directory ‘result’: File exists\r\n",
      "evaluate on data amr_simple_test\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:11<00:00,  6.84it/s]\r\n",
      "sample:  balon tersebut ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  obe sedang menulis puisi ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak yang ajaib itu bernama angga ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia berenang ---- dia sedang berenang\r\n",
      "sample:  buku yang hilang dicari oleh ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibu sedang menyapu halaman rumah ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi yang ayah bajak di sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  di tepi pantai, andi pergi ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  32.137943149928766\r\n",
      "evaluate on data b-salah-darat\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  3.08it/s]\r\n",
      "sample:  kantor kantor sama itu sedang berdiskusi di bandara soekarnohatta mei 10 2016 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarnohatta alif siang ini sedang meeting soal kepada sabtu 14 mei ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  pada pukul 18, anak natalie berangkat ke singapura ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  kata sekretaris agus haryadi di perusahaan pt angkasa ii, akan melakukan laku kelola bandara internasional soekarnohatta ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  di remote area, pesawat parkir bersiap-siap ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  di padang, sopir jemput oleh edward ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak yang melakukan investigasi ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  kemenhub memberikan sanksi kepada pihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  cerita teman tersebut ditulis oleh zara di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  di remote area, pesawat parkir bersiap-siap ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  7.779637653277772\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  3.43it/s]\r\n",
      "sample:  kendara laju arah mal bintaro xchange akan melakukan flyover di atas lampu merah ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  terjadi kecelakan tanah dan gedung miring itu terjadi di menara jakarta timur ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  di tangerang selatan, terjadi kecelakan gedung itu ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  pembangunan gedung itu putus oleh panin karena lulus yang layak ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung itu dibangun oleh lulus tidak sehingga pembangunan gedung itu putus ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kata kasat akp samian di polres tangerang selatan, pembangunan gedung itu dilakukan oleh pihak yang tahu saat ini ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  kami sedang meliput peristiwa yang sebut tadi ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  terjadi kecelekaan oleh korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  rugi material yang terjadi ---- terjadi kerugian material.\r\n",
      "sample:  di tangerang selatan, terjadi kecelakan gedung ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  7.49138735703375\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:02<00:00,  2.96it/s]\r\n",
      "sample:  kejadian ini ditandatangani oleh indosat ooredoo indonesia dan langgan bisnis itu hadir dalam bentuk smart internet ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kejadian ini ditandatangani oleh indosat ooredoo indonesia dan langgan bisnis itu hadir dalam bentuk smart internet ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  smart mobility iot dan peningkatan aset akan dilakukan oleh perusahaan itu ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  kejadian ini ditandatangani oleh indosat ooredoo indonesia dan langgan bisnis itu hadir dalam bentuk smart internet ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan oleh perusahaan itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president achmad sofwan memimpin perusahaan realtime dan mobilitas akan dilakukan dengan cara yang optimal ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan oleh perusahaan itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  kejadian ini dikerjakan oleh indosat ooredoo dan fujitsu indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president achmad sofwan memimpin perusahaan realtime dan mobilitas dilakukan dengan cara optimal ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan oleh perusahaan itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  0.6374410792131114\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:02<00:00,  2.89it/s]\r\n",
      "sample:  ciptadi melompat di jembatan yang tinggi 15 meter ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang disampaikan oleh kasubag reny kompol di mapolrestabes bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  di kota bandung, rezha telah bubar salat di masjid raya bandung jumat dan ia meledakkan megatron di depan kantor ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  di atas jpo, tugas linmas ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri dilakukan di alun alun ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  orang tua terjatuh dalam diri di alun bandung ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  di alun alun, sudirman menjadi saksi mata anggota linmas di kota bandung ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, lelaki itu terjatuh di jembatan seberang jumat ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  hidup itu diakhiri oleh lelaki tengah baya di tengah jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  di bandung, seorang pria misterius membuat bom di seberang jalan ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  6.409873137202585\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  2.84it/s]\r\n",
      "sample:  stasiun banjarnegara berada di jawa tengah badan meteorologi geofisika pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  di desa kecamatan batang, pusat gempa berada tanji gugur ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  di desa sumberejo, terdapat tutup sementara ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala surono memimpin pusat vulkanologi mitigasi bencana geologi pukul 19 00 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  tutup dilakukan oleh tim ukur di lapangan ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  di desa sumberejo, terdapat tutup sementara ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  tugas pusat vulkanologi mitigasi bencana geologi yang diberikan oleh pantau kawah timbang ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  di pusat vulkanologi mitigasi bencana geologi, terjadi gempa pukul 19 00 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi tinggal di kabupaten banjarnegara sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  stasiun banjarnegara diresmikan oleh badan meteorologi klimatologi geofisika pukul 19 ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  7.697201928691179\r\n",
      "mv: cannot move 'result/linearized_penman' to a subdirectory of itself, 'result/linearized_penman/linearized_penman'\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x ./evaluate_indoBART_to_all.sh\n",
    "!mkdir result\n",
    "!./evaluate_indoBART_to_all.sh ../train/result linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb48e4fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:14:32.405631Z",
     "iopub.status.busy": "2022-05-24T00:14:32.404871Z",
     "iopub.status.idle": "2022-05-24T00:14:33.054187Z",
     "shell.execute_reply": "2022-05-24T00:14:33.053700Z",
     "shell.execute_reply.started": "2022-03-14T06:46:21.715669Z"
    },
    "papermill": {
     "duration": 1.797878,
     "end_time": "2022-05-24T00:14:33.054350",
     "exception": false,
     "start_time": "2022-05-24T00:14:31.256472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_generations.txt (deflated 61%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_generations.txt (deflated 84%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_generations.txt (deflated 63%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_without_sta.zip result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c87e6",
   "metadata": {
    "papermill": {
     "duration": 1.198689,
     "end_time": "2022-05-24T00:14:35.378592",
     "exception": false,
     "start_time": "2022-05-24T00:14:34.179903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune indobart with linearized penman + tree level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a297f88c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:14:37.897804Z",
     "iopub.status.busy": "2022-05-24T00:14:37.897008Z",
     "iopub.status.idle": "2022-05-24T00:14:37.900577Z",
     "shell.execute_reply": "2022-05-24T00:14:37.901146Z"
    },
    "papermill": {
     "duration": 1.204236,
     "end_time": "2022-05-24T00:14:37.901346",
     "exception": false,
     "start_time": "2022-05-24T00:14:36.697110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/tree_level_embeddings\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/tree_level_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b94b0a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:14:40.226354Z",
     "iopub.status.busy": "2022-05-24T00:14:40.225596Z",
     "iopub.status.idle": "2022-05-24T00:14:40.870016Z",
     "shell.execute_reply": "2022-05-24T00:14:40.869161Z"
    },
    "papermill": {
     "duration": 1.815685,
     "end_time": "2022-05-24T00:14:40.870143",
     "exception": false,
     "start_time": "2022-05-24T00:14:39.054458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bf0275e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:14:43.133441Z",
     "iopub.status.busy": "2022-05-24T00:14:43.132663Z",
     "iopub.status.idle": "2022-05-24T00:18:18.111297Z",
     "shell.execute_reply": "2022-05-24T00:18:18.110762Z"
    },
    "papermill": {
     "duration": 216.114082,
     "end_time": "2022-05-24T00:18:18.111443",
     "exception": false,
     "start_time": "2022-05-24T00:14:41.997361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Some weights of MBartForConditionalGeneration were not initialized from the model checkpoint at indobenchmark/indobart and are newly initialized: ['model.encoder.tree_embed.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:1.3870 LR:0.00003000: 100%|█| 662/662 [00:45<00:00, 14.53it\r\n",
      "(Epoch 1) DEV LOSS:0.4111 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 43.72it/s]\r\n",
      "bleu score on dev:  58.45206044614471\r\n",
      "(Epoch 2) TRAIN LOSS:0.5565 LR:0.00003000: 100%|█| 662/662 [00:46<00:00, 14.35it\r\n",
      "(Epoch 2) DEV LOSS:0.5936 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 42.65it/s]\r\n",
      "bleu score on dev:  52.416784417341034\r\n",
      "(Epoch 3) TRAIN LOSS:0.4405 LR:0.00003000: 100%|█| 662/662 [00:46<00:00, 14.37it\r\n",
      "(Epoch 3) DEV LOSS:0.5785 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 43.39it/s]\r\n",
      "bleu score on dev:  40.23496367556997\r\n",
      "(Epoch 4) TRAIN LOSS:0.3927 LR:0.00003000: 100%|█| 662/662 [00:47<00:00, 14.07it\r\n",
      "(Epoch 4) DEV LOSS:0.5856 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 42.95it/s]\r\n",
      "bleu score on dev:  57.13385738234364\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.20it/s]\r\n",
      "sample:  balon ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak ajaib itu bernama ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  32.03378873198283\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman_with_tree_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9b2580e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:18:22.311410Z",
     "iopub.status.busy": "2022-05-24T00:18:22.310544Z",
     "iopub.status.idle": "2022-05-24T00:18:22.956992Z",
     "shell.execute_reply": "2022-05-24T00:18:22.957472Z"
    },
    "papermill": {
     "duration": 2.75308,
     "end_time": "2022-05-24T00:18:22.957628",
     "exception": false,
     "start_time": "2022-05-24T00:18:20.204548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x ./evaluate_indoBART_to_all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbfa9aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:18:27.527911Z",
     "iopub.status.busy": "2022-05-24T00:18:27.526538Z",
     "iopub.status.idle": "2022-05-24T00:20:19.853378Z",
     "shell.execute_reply": "2022-05-24T00:20:19.853816Z"
    },
    "papermill": {
     "duration": 114.748257,
     "end_time": "2022-05-24T00:20:19.853982",
     "exception": false,
     "start_time": "2022-05-24T00:18:25.105725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder result\r\n",
      "preprocessing method linearized_penman_with_tree_level\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 167205.02it/s]\r\n",
      "total: 306  tuple_sent_amr_level\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 118509.91it/s]\r\n",
      "total: 32  tuple_sent_amr_level\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 606/606 [00:00<00:00, 124975.33it/s]\r\n",
      "total: 29  tuple_sent_amr_level\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 128458.64it/s]\r\n",
      "total: 27  tuple_sent_amr_level\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 131988.59it/s]\r\n",
      "total: 23  tuple_sent_amr_level\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 128719.05it/s]\r\n",
      "total: 19  tuple_sent_amr_level\r\n",
      "evaluate on data amr_simple_test\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.34it/s]\r\n",
      "sample:  balon ditiup oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  anak ajaib itu bernama ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia berenang ---- dia sedang berenang\r\n",
      "sample:  buku hilang dicari ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibu sedang menyapu halaman rumah ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi ditanam ayah di sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  di tepi pantai, andi pergi ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  32.03378873198283\r\n",
      "evaluate on data b-salah-darat\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.50it/s]\r\n",
      "sample:  di kantor pt pt angkasa pura ii, di bandara soekarnohatta, terjadi insiden terbang yang tiba di jakarta dari mei 2016 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  di kantor imigrasi soekarnohatta, kami bertemu pada sabtu ini ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak natalie berangkat ke pesawat lion air jt 161 pada pukul 18 55 ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  direktur haryadi seorang perusahaan pt angkasa pura ii yang melakukan laku kelola bandara internasional soekarnohatta ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  pesawat parkir itu disiapkan oleh kami di remote area ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  dari padang, edward mendapatkan pekerjaan sebagai sopir jemput bus ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak bandara soetta yang memanggil pihak kait-nasional ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  kemenhub memberikan sanksi kepada pihakpihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  zara menulis cerita temannya di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  pesawat parkir itu disiapkan oleh kami di remote area ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  9.879791521839332\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  3.05it/s]\r\n",
      "sample:  kendara itu bergerak semakin arah mal bintaro xchange di atas jalan boulevard bintaro ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kejadian ini terjadi ketika pembangunan gedung miring dan di jalan jakarta timur, gedung itu roboh ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  di tangerang selatan, terjadi peristiwa roboh gedung itu ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  pembangunan gedung itu putus karena tidak lulus dengan layak ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung itu dibangun dengan lulus terbaik dan pembangunan gedung itu ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kasat akp samian di polres tangerang selatan melakukan laku tahu di gedung itu saat ini ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  sejak ini, kami melihat peristiwa yang sebut ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  kejadian ini terjadi ketika korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  rugi material yang terjadi ---- terjadi kerugian material.\r\n",
      "sample:  di tangerang selatan, terjadi bongkar di gedung itu ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  5.241249340945514\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:03<00:00,  2.19it/s]\r\n",
      "sample:  sertifikat yang diberikan oleh indosat fujitsu indonesia kepada para mitra bangun rumah itu adalah dalam teknologi internet of things ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  sertifikat yang diberikan oleh indosat fujitsu indonesia kepada para mitra bangun rumah itu adalah dalam teknologi internet of things ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  teknologi iot yang mereka gunakan merupakan yang signifikan dan perbaikan akan berlangsung secara keseluruhan ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  sertifikat yang diberikan oleh indosat fujitsu indonesia kepada para mitra bangun rumah itu adalah dalam teknologi internet of things ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan di sektor industri itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president sofwan memimpin layanan data realtime di indonesia dan mobilitas merupakan cara yang optimal dalam meningkatkan strategi usaha ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan di sektor industri itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  indosat ooredoo yang hadir dalam solusi smart mobility di indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president sofwan merupakan director fujitsu dan mobilitas merupakan cara yang optimal dalam perbaikan data realtime ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  di indonesia, kerja sama akan dilakukan di sektor industri itu ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  1.7696994497955176\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:02<00:00,  2.42it/s]\r\n",
      "sample:  di jembatan yang kira-rangi oleh 15 meter, ia melompat ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang sebut terjadi di kasubag reny 01 di bandung pada saat temu jumat di mapolrestabes bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  di bandung, rezha warga kota bandung dibela oleh noviana yang telah bubar di masjid raya bandung jumat ini ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  satpam linmas berdiri di atas jpo ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di alun bandung pada saat ini ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  di alun-alangi kerumunan orang tua, dirinya terjatuh ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  di alun bandung, seorang saksi mata meledakkan diri di anggota linmas di kota bandung ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, seorang orang itu terjatuh di jembatan seberang ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  di tengah malam ini, ia mengakhiri hidup di jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  di bandung, seorang pria misterius membuat jembatan di bandung barat, ia membuat pesan di seberang orang-orang ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  8.354429364529265\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  2.52it/s]\r\n",
      "sample:  stasiun banjarnegara jawa tengah yang dibangun oleh badan meteorologi klimatologi geofisika pada pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  ada pusat gempa di desa kecamatan batang di kabupaten batang ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  tutup sementara yang pvmbg umumkan di jalan dekat kawah di desa sumberejo ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala vulkanologi bencana geologi berkata ada banyak gempa yang terjadi pukul 19 00 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  tutup dilakukan oleh tim ukur di lapangan hingga ada gas bahaya ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  tutup sementara yang pvmbg umumkan di jalan dekat kawah di desa sumberejo ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  di pusat vulkanologi bencana geologi, terjadi peningkatan aktivitas kawah timbang dan pemerintah memberikan informasi ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  di pusat vulkanologi mitigasi bencana geologi, terjadi gempa dan gempa terjadi sejak pukul 19 00 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi mengungsi ke kabupaten banjarnegara pada sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  stasiun meteorologi klimatologi geofisika yang melakukan guncang gempa kuat 4 skala richter pada pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  11.91103574344951\r\n"
     ]
    }
   ],
   "source": [
    "!./evaluate_indoBART_to_all.sh result linearized_penman_with_tree_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f567ff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:20:24.726902Z",
     "iopub.status.busy": "2022-05-24T00:20:24.726138Z",
     "iopub.status.idle": "2022-05-24T00:20:53.304946Z",
     "shell.execute_reply": "2022-05-24T00:20:53.305364Z"
    },
    "papermill": {
     "duration": 30.723571,
     "end_time": "2022-05-24T00:20:53.305533",
     "exception": false,
     "start_time": "2022-05-24T00:20:22.581962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 61%)\r\n",
      "  adding: result/test_label.txt (deflated 60%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/loss_data.tsv (deflated 34%)\r\n",
      "  adding: result/model/ (stored 0%)\r\n",
      "  adding: result/model/config.json (deflated 63%)\r\n",
      "  adding: result/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 82%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/test_generations.txt (deflated 63%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_without_sta.zip result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f609f601",
   "metadata": {
    "papermill": {
     "duration": 2.625593,
     "end_time": "2022-05-24T00:20:58.346946",
     "exception": false,
     "start_time": "2022-05-24T00:20:55.721353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune indobart with linearized penman + supervised task adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73888b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:21:03.045120Z",
     "iopub.status.busy": "2022-05-24T00:21:03.044240Z",
     "iopub.status.idle": "2022-05-24T00:21:03.047595Z",
     "shell.execute_reply": "2022-05-24T00:21:03.048025Z"
    },
    "papermill": {
     "duration": 2.142145,
     "end_time": "2022-05-24T00:21:03.048155",
     "exception": false,
     "start_time": "2022-05-24T00:21:00.906010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/train\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "641f21d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:21:07.397838Z",
     "iopub.status.busy": "2022-05-24T00:21:07.397094Z",
     "iopub.status.idle": "2022-05-24T00:21:08.048084Z",
     "shell.execute_reply": "2022-05-24T00:21:08.047566Z"
    },
    "papermill": {
     "duration": 2.814672,
     "end_time": "2022-05-24T00:21:08.048236",
     "exception": false,
     "start_time": "2022-05-24T00:21:05.233564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'result/model': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm result/* -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ecde109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:21:12.610843Z",
     "iopub.status.busy": "2022-05-24T00:21:12.610083Z",
     "iopub.status.idle": "2022-05-24T01:13:07.800037Z",
     "shell.execute_reply": "2022-05-24T01:13:07.800639Z"
    },
    "papermill": {
     "duration": 3117.579417,
     "end_time": "2022-05-24T01:13:07.800818",
     "exception": false,
     "start_time": "2022-05-24T00:21:10.221401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing method linearized_penman\r\n",
      "silver data folder ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/preprocessed_silver_data/train.amr.txt', result_sent_path='data/preprocessed_silver_data/train.sent.txt', source_file_path=None, source_folder_path='../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news')\r\n",
      "100%|███████████████████████████████| 265325/265325 [00:01<00:00, 143425.80it/s]\r\n",
      "100%|███████████████████████████████| 270251/270251 [00:01<00:00, 149268.91it/s]\r\n",
      "100%|███████████████████████████████| 263508/263508 [00:01<00:00, 151680.74it/s]\r\n",
      "100%|███████████████████████████████| 268572/268572 [00:01<00:00, 149478.39it/s]\r\n",
      "100%|███████████████████████████████| 264253/264253 [00:02<00:00, 131028.86it/s]\r\n",
      "100%|███████████████████████████████| 266966/266966 [00:01<00:00, 148866.20it/s]\r\n",
      "100%|███████████████████████████████| 268262/268262 [00:01<00:00, 147831.86it/s]\r\n",
      "100%|███████████████████████████████| 269258/269258 [00:01<00:00, 149544.70it/s]\r\n",
      "100%|███████████████████████████████| 264330/264330 [00:02<00:00, 117341.26it/s]\r\n",
      "100%|███████████████████████████████| 263263/263263 [00:01<00:00, 150606.34it/s]\r\n",
      "total: 154892 pair sent-amr\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  154892\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  38723\r\n",
      "(Epoch 1) TRAIN LOSS:1.0775 LR:0.00003000: 100%|█| 38723/38723 [51:07<00:00, 12.\r\n",
      "(Epoch 1) DEV LOSS:3.3653 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 51.62it/s]\r\n",
      "bleu score on dev:  14.048501104445915\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:08<00:00,  8.70it/s]\r\n",
      "sample:  ilham tiup balon. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya ketik makalah. ---- saya mengetik makalah\r\n",
      "sample:  angga anak ajaib. ---- angga anak ajaib\r\n",
      "sample:  ibunya orang dosen. ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  33.53909394510186\r\n",
      "tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\r\n",
      "           384,   384,   384,     2, 40002]])\r\n",
      "saya ketik makalah.\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x train_indoBART_on_silver_data.sh\n",
    "!./train_indoBART_on_silver_data.sh linearized_penman ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbdf8a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:13:44.372799Z",
     "iopub.status.busy": "2022-05-24T01:13:44.372044Z",
     "iopub.status.idle": "2022-05-24T01:13:45.021172Z",
     "shell.execute_reply": "2022-05-24T01:13:45.021898Z"
    },
    "papermill": {
     "duration": 19.099983,
     "end_time": "2022-05-24T01:13:45.022078",
     "exception": false,
     "start_time": "2022-05-24T01:13:25.922095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu_score_test.txt  loss_data.tsv  model  test_generations.txt  test_label.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls result/result_supervised_task_adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "584efa90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:14:20.450969Z",
     "iopub.status.busy": "2022-05-24T01:14:20.450191Z",
     "iopub.status.idle": "2022-05-24T01:14:21.097805Z",
     "shell.execute_reply": "2022-05-24T01:14:21.097305Z"
    },
    "papermill": {
     "duration": 18.569443,
     "end_time": "2022-05-24T01:14:21.097929",
     "exception": false,
     "start_time": "2022-05-24T01:14:02.528486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a42b1a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:14:56.416107Z",
     "iopub.status.busy": "2022-05-24T01:14:56.415373Z",
     "iopub.status.idle": "2022-05-24T01:18:14.479118Z",
     "shell.execute_reply": "2022-05-24T01:18:14.478253Z"
    },
    "papermill": {
     "duration": 216.0223,
     "end_time": "2022-05-24T01:18:14.479290",
     "exception": false,
     "start_time": "2022-05-24T01:14:38.456990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "resume from checkpoint\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:0.9176 LR:0.00003000: 100%|█| 662/662 [00:39<00:00, 16.77it\r\n",
      "(Epoch 1) DEV LOSS:1.0518 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 59.79it/s]\r\n",
      "bleu score on dev:  33.79805041124448\r\n",
      "(Epoch 2) TRAIN LOSS:0.4803 LR:0.00003000: 100%|█| 662/662 [00:39<00:00, 16.79it\r\n",
      "(Epoch 2) DEV LOSS:0.9702 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 55.36it/s]\r\n",
      "bleu score on dev:  46.202791064908276\r\n",
      "(Epoch 3) TRAIN LOSS:0.3951 LR:0.00003000: 100%|█| 662/662 [00:45<00:00, 14.51it\r\n",
      "(Epoch 3) DEV LOSS:0.9453 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 36.73it/s]\r\n",
      "bleu score on dev:  45.20398976762332\r\n",
      "(Epoch 4) TRAIN LOSS:0.3670 LR:0.00003000: 100%|█| 662/662 [00:43<00:00, 15.30it\r\n",
      "(Epoch 4) DEV LOSS:0.8507 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 58.87it/s]\r\n",
      "bleu score on dev:  38.73933119641338\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.32it/s]\r\n",
      "sample:  balon tersebut dipukul oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  makalah diketik oleh saya ---- saya mengetik makalah\r\n",
      "sample:  andi anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu adalah seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  36.258432634140064\r\n",
      "tensor([[    0,   382,  9338, 40007,   382,   475,   384, 40008,   382,  6353,\r\n",
      "           384,   384,   384,     2, 40002]])\r\n",
      "makalah diketik oleh saya\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman  --result_folder result/result_linearized_penman --resume_from_checkpoint True --saved_model_folder_path result/result_supervised_task_adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0e54dc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:18:52.169155Z",
     "iopub.status.busy": "2022-05-24T01:18:52.168424Z",
     "iopub.status.idle": "2022-05-24T01:19:20.889805Z",
     "shell.execute_reply": "2022-05-24T01:19:20.889298Z"
    },
    "papermill": {
     "duration": 47.89509,
     "end_time": "2022-05-24T01:19:20.889950",
     "exception": false,
     "start_time": "2022-05-24T01:18:32.994860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/result_linearized_penman/ (stored 0%)\r\n",
      "  adding: result/result_linearized_penman/test_label.txt (deflated 60%)\r\n",
      "  adding: result/result_linearized_penman/loss_data.tsv (deflated 34%)\r\n",
      "  adding: result/result_linearized_penman/model/ (stored 0%)\r\n",
      "  adding: result/result_linearized_penman/model/config.json (deflated 63%)\r\n",
      "  adding: result/result_linearized_penman/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/result_linearized_penman/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/result_linearized_penman/test_generations.txt (deflated 63%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_with_sta.zip result/result_linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dab57cbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:19:58.188614Z",
     "iopub.status.busy": "2022-05-24T01:19:58.187785Z",
     "iopub.status.idle": "2022-05-24T01:19:58.190782Z",
     "shell.execute_reply": "2022-05-24T01:19:58.191183Z"
    },
    "papermill": {
     "duration": 18.282024,
     "end_time": "2022-05-24T01:19:58.191339",
     "exception": false,
     "start_time": "2022-05-24T01:19:39.909315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/evaluate\n"
     ]
    }
   ],
   "source": [
    "%cd ../evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "599b71a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:20:35.033958Z",
     "iopub.status.busy": "2022-05-24T01:20:35.033172Z",
     "iopub.status.idle": "2022-05-24T01:20:35.795726Z",
     "shell.execute_reply": "2022-05-24T01:20:35.795153Z"
    },
    "papermill": {
     "duration": 18.704599,
     "end_time": "2022-05-24T01:20:35.795852",
     "exception": false,
     "start_time": "2022-05-24T01:20:17.091253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'result/linearized_penman': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm result/* -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51c150a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:21:12.826042Z",
     "iopub.status.busy": "2022-05-24T01:21:12.825311Z",
     "iopub.status.idle": "2022-05-24T01:23:08.907457Z",
     "shell.execute_reply": "2022-05-24T01:23:08.907860Z"
    },
    "papermill": {
     "duration": 134.491035,
     "end_time": "2022-05-24T01:23:08.908047",
     "exception": false,
     "start_time": "2022-05-24T01:20:54.417012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder ../train/result/result_linearized_penman\r\n",
      "preprocessing method linearized_penman\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 102439.43it/s]\r\n",
      "total: 306 pair sent-amr\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 676/676 [00:00<00:00, 54424.43it/s]\r\n",
      "total: 32 pair sent-amr\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 606/606 [00:00<00:00, 62334.42it/s]\r\n",
      "total: 29 pair sent-amr\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 745/745 [00:00<00:00, 70992.99it/s]\r\n",
      "total: 27 pair sent-amr\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 468/468 [00:00<00:00, 73680.95it/s]\r\n",
      "total: 23 pair sent-amr\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|██████████████████████████████████████| 412/412 [00:00<00:00, 76908.33it/s]\r\n",
      "total: 19 pair sent-amr\r\n",
      "mkdir: cannot create directory ‘result’: File exists\r\n",
      "evaluate on data amr_simple_test\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:11<00:00,  6.49it/s]\r\n",
      "sample:  balon tersebut dipukul oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  makalah diketik oleh saya ---- saya mengetik makalah\r\n",
      "sample:  andi anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu adalah seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia sedang berenang ---- dia sedang berenang\r\n",
      "sample:  buku yang hilang dicari oleh ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  ibu menyapu halaman rumah ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi yang ayah bajak di sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  andi pergi berlibur di tepi pantai ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  36.258432634140064\r\n",
      "evaluate on data b-salah-darat\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.07it/s]\r\n",
      "sample:  kantor kantor sama dan pt angkasa pura ii akan membahas insiden penerbangan pesawat yang tiba di jakarta dari singapura 10 mei 2016 di lion air jt 161 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarnta alif suadi ujarnya kepada tempo sabtu malam, kami meeting soal ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak teman natalie berangkat dan menggunakan pesawat lion air pukul 18 55 ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  sekretaris agusyadi mengatakan, perusahaan pt angkasa pura ii melakukan pengelolaan bandara internasional soekarnohatta pihak yang akan berkoordinasi kantor otoritas bandara wilayah 1 terkait perisitiwa ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  di remote area, kami menyiapkan bus antar penumpang di terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  di padang, edward berkata terdapat sopir yang menjemput penumpang di bus ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak terkait dan melakukan investigasi ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  dinda memberikan sanksi kepada pihak-pihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  lisa berkata cerita temannya jika pesawat itu tidak mendarat di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  di remote area, kami menyiapkan bus antar penumpang di terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  20.894887144397686\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.30it/s]\r\n",
      "sample:  kendara melaju ke arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dilakukan dengan menggunakan flyover di atas lampu indomobil merah ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kejadian itu mengakibatkan gedung miring dan salah satu tes tanah di menara timur jalan mt haryono saidah ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  kapolres ayi supardan dari tangerang selatan membenarkan peristiwa bintaro yang roboh gedung itu menarik hati masyarakat banyak ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  panin memutuskan melanjutkan pembangunan gedung itu dikarenakan tidak lulus uji kelayakan ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung itu disebutkan tidak lulus ujian sehingga pembangunan gedung itu memutuskan untuk tidak melanjutkan pembangunan ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  saat ini, kasat akp samian dari tangerang selatan mengatakan, kasat reskrim melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung itu ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  kami sedang mendalami peristiwa yang kami sebut sampai ini ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  kejadian itu tidak menimbulkan korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  menjadi kerugian material ---- terjadi kerugian material.\r\n",
      "sample:  di sektor bintaro, di tangerang selatan, gedung itu diduga menyalahi prosedur ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  22.57927387536413\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:03<00:00,  1.81it/s]\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman yang mendalam dalam rangka meningkatkan mitra pembangunan telah lama, pelanggan diajak langgan bisnis secara mendalam, hadir solusi smart dan smart internet of mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman yang mendalam dalam rangka meningkatkan mitra pembangunan telah lama, pelanggan diajak langgan bisnis secara mendalam, hadir solusi smart dan smart internet of mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  pada smart mobility dan jaraknya cukup signifikan, dimulai mengubah cara usaha dalam mengelola aset ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  indosat bersama fujitsu indonesia menandatangani nota kesepahaman yang mendalam dalam rangka meningkatkan mitra pembangunan telah lama, pelanggan diajak langgan bisnis secara mendalam, hadir solusi smart dan smart internet of mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kerja sama itu terfokus pada sektor transportasi dan otomotif secara luas dan mampu memenuhi pelanggan korporasi di indonesia ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  di director achmad indonesia, president achmad sofwan menuturkan, menyediakan layanan proses data realtime dan aplikasi mobilitas memanfaatkan cara optimal strategi perusahaan itu ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  kerja sama itu terfokus pada sektor transportasi dan otomotif secara luas dan mampu memenuhi pelanggan korporasi di indonesia ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  indosat ooredoo mengerjakan solusi smart mobility internet of things dan bekerja sama dengan fujitsu di indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  di director achmad, president achmad mengungkapkan bahwa president achmad memanfaatkan layanan yang realtime dan aplikasi itu memungkinkan mobilitasnya dengan cara optimal strategi perusahaan itu ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  kerja sama itu terfokus pada sektor transportasi dan otomotif secara luas dan mampu memenuhi pelanggan korporasi di indonesia ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  11.362258459581273\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:03<00:00,  1.96it/s]\r\n",
      "sample:  selebadi melompat di megatron jembatan yang ketinggiannya kira 15 meter sebrang orang ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang sebut itu diungkapkan kasubag reny marthaliana kepada humas kompol di kabag dede rojudin kompol yang ditemui jumat di mapolrestabes bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  rezha warga kota bandung kepada noviana mengatakan telah dibubarkan salat di masjid raya bandung jumat dan para jemaat lain di depan kantor pos besar di jembatan seberang orang ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  petugas linmas berdiri dan satpam berapa di atas jpo ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di alun bandung alun itu belum lama ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  orang tua terjatuh diri di jpo di alun alun kota bandung ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  menjadi saksi mata bunuh diri di alun bandung, dirinya mengatakan kepada anggota linmas kota bandung satpol pp, dirinya memperkirakan pria itu memperkirakan 30 tahun ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, dirinya terjatuh di depan pos giro besar bandung di jembatan seberang, pada jumat ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  di tengah malam, hidup diakhiri oleh lelaki setengah baya lalu lintas di jalan asia afrika ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  pria itu membuat pesanan bunuh diri di jembatan bandung, jawa barat, di seberang orang-orang itu sedang membuat pesanan untuk belum terjun nekat ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  20.53136494153066\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"../train/result/result_linearized_penman/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:02<00:00,  1.88it/s]\r\n",
      "sample:  stasiun banjarnegara di jawa tengah (badan meteorologi klimatologi geofisika) secara geofisika menyatakan gempa berkekuatan 4 8 skala richter di dataran tinggi dieng pukul 19: 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  di desa di kecamatan batang, pusat gempa diperkirakan ada di desa tanji gugur ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  pvmbg rekomendasi untuk menutup sementara jalan yang dekat kawah di desa sumberejo secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala surono mengatakan, kepala pusat vulkanologi mitigasi bencana geologi mengatakan banyak 86 kali gempa yang terjadi pukul 19 00 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  tutup dilakukan oleh tim mengukur konsentrasi gas di lapangan dengan tidak ada gas berbahaya ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  pvmbg rekomendasi untuk menutup sementara jalan yang dekat kawah di desa sumberejo secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  petugas pusat vulkanologi mitigasi bencana geologi menerimaku untuk memantau aktivitas kawah timbang dan memberikan informasi ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  data yang dikeluarkan pusat vulkanologi mitigasi bencana geologi menyebutkan dan telah terdengar gempa sebanyak 86 kali dan gempa dimulai pukul 19 00 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi di kabupaten banjarnegara meninggal pengungsi sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  di banjarnegara, stasiun badan meteorologiatologi geofisika geofisika geofisika secara geofisika menyatakan guncangan gempa berkekuatan 4 8 skala richter di dataran tinggi dieng pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  24.645623728527244\r\n",
      "mkdir: cannot create directory ‘result/linearized_penman’: File exists\r\n",
      "mv: cannot move 'result/amr_simple_test' to 'result/linearized_penman/amr_simple_test': Directory not empty\r\n",
      "mv: cannot move 'result/b-salah-darat' to 'result/linearized_penman/b-salah-darat': Directory not empty\r\n",
      "mv: cannot move 'result/c-gedung-roboh' to 'result/linearized_penman/c-gedung-roboh': Directory not empty\r\n",
      "mv: cannot move 'result/d-indo-fuji' to 'result/linearized_penman/d-indo-fuji': Directory not empty\r\n",
      "mv: cannot move 'result/f-bunuh-diri' to 'result/linearized_penman/f-bunuh-diri': Directory not empty\r\n",
      "mv: cannot move 'result/g-gempa-dieng' to 'result/linearized_penman/g-gempa-dieng': Directory not empty\r\n",
      "mv: cannot move 'result/linearized_penman' to a subdirectory of itself, 'result/linearized_penman/linearized_penman'\r\n"
     ]
    }
   ],
   "source": [
    "!./evaluate_indoBART_to_all.sh ../train/result/result_linearized_penman linearized_penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a127a9bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:23:46.065826Z",
     "iopub.status.busy": "2022-05-24T01:23:46.065074Z",
     "iopub.status.idle": "2022-05-24T01:23:46.717344Z",
     "shell.execute_reply": "2022-05-24T01:23:46.718057Z"
    },
    "papermill": {
     "duration": 19.551183,
     "end_time": "2022-05-24T01:23:46.718255",
     "exception": false,
     "start_time": "2022-05-24T01:23:27.167072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/f-bunuh-diri/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/c-gedung-roboh/test_generations.txt (deflated 61%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/amr_simple_test/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/b-salah-darat/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/d-indo-fuji/test_generations.txt (deflated 84%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/linearized_penman/g-gempa-dieng/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 62%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 65%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 82%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 65%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_with_sta.zip result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a15eccde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:24:24.069747Z",
     "iopub.status.busy": "2022-05-24T01:24:24.068988Z",
     "iopub.status.idle": "2022-05-24T01:24:24.750966Z",
     "shell.execute_reply": "2022-05-24T01:24:24.750475Z"
    },
    "papermill": {
     "duration": 19.110765,
     "end_time": "2022-05-24T01:24:24.751091",
     "exception": false,
     "start_time": "2022-05-24T01:24:05.640326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_indoBART.py\t     result_without_sta.zip\r\n",
      "evaluate_indoBART_to_all.sh  zeroshot_evaluate_indoBART.py\r\n",
      "evaluate_indoT5.py\t     zeroshot_evaluate_indoBART_to_all.sh\r\n",
      "evaluate_indoT5_to_all.sh    zeroshot_evaluate_indoT5.py\r\n",
      "result\t\t\t     zeroshot_evaluate_indoT5_to_all.sh\r\n",
      "result_with_sta.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc81fa",
   "metadata": {
    "papermill": {
     "duration": 18.293354,
     "end_time": "2022-05-24T01:25:02.204121",
     "exception": false,
     "start_time": "2022-05-24T01:24:43.910767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## linearized penman + tree level embeddings + supervised task adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50e8cee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:25:39.156603Z",
     "iopub.status.busy": "2022-05-24T01:25:39.155887Z",
     "iopub.status.idle": "2022-05-24T01:25:39.158951Z",
     "shell.execute_reply": "2022-05-24T01:25:39.159379Z"
    },
    "papermill": {
     "duration": 18.262628,
     "end_time": "2022-05-24T01:25:39.159521",
     "exception": false,
     "start_time": "2022-05-24T01:25:20.896893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/amr-to-text-indonesia/tree_level_embeddings\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/amr-to-text-indonesia/tree_level_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23cc0ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:26:17.187941Z",
     "iopub.status.busy": "2022-05-24T01:26:17.187150Z",
     "iopub.status.idle": "2022-05-24T01:26:17.833059Z",
     "shell.execute_reply": "2022-05-24T01:26:17.832303Z"
    },
    "papermill": {
     "duration": 19.52689,
     "end_time": "2022-05-24T01:26:17.833233",
     "exception": false,
     "start_time": "2022-05-24T01:25:58.306343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'result/amr_simple_test': Is a directory\r\n",
      "rm: cannot remove 'result/b-salah-darat': Is a directory\r\n",
      "rm: cannot remove 'result/c-gedung-roboh': Is a directory\r\n",
      "rm: cannot remove 'result/d-indo-fuji': Is a directory\r\n",
      "rm: cannot remove 'result/f-bunuh-diri': Is a directory\r\n",
      "rm: cannot remove 'result/g-gempa-dieng': Is a directory\r\n",
      "rm: cannot remove 'result/model': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm result/* -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39042b72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T01:26:55.361380Z",
     "iopub.status.busy": "2022-05-24T01:26:55.360640Z",
     "iopub.status.idle": "2022-05-24T02:33:23.995213Z",
     "shell.execute_reply": "2022-05-24T02:33:23.957847Z"
    },
    "papermill": {
     "duration": 4007.981729,
     "end_time": "2022-05-24T02:33:23.995377",
     "exception": false,
     "start_time": "2022-05-24T01:26:36.013648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing method linearized_penman_with_tree_level\r\n",
      "silver data folder ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news\r\n",
      "mkdir: cannot create directory ‘data/preprocessed_silver_data’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/preprocessed_silver_data/train.amr.txt', result_sent_path='data/preprocessed_silver_data/train.sent.txt', source_file_path=None, source_folder_path='../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news')\r\n",
      "100%|███████████████████████████████| 265325/265325 [00:01<00:00, 148519.51it/s]\r\n",
      "16077\r\n",
      "100%|███████████████████████████████| 270251/270251 [00:02<00:00, 130026.38it/s]\r\n",
      "14986\r\n",
      "100%|███████████████████████████████| 263508/263508 [00:01<00:00, 145535.23it/s]\r\n",
      "15926\r\n",
      "100%|███████████████████████████████| 268572/268572 [00:01<00:00, 148349.83it/s]\r\n",
      "14937\r\n",
      "100%|███████████████████████████████| 264253/264253 [00:02<00:00, 128296.50it/s]\r\n",
      "16046\r\n",
      "100%|███████████████████████████████| 266966/266966 [00:01<00:00, 144090.55it/s]\r\n",
      "14930\r\n",
      "100%|███████████████████████████████| 268262/268262 [00:01<00:00, 148288.21it/s]\r\n",
      "14991\r\n",
      "100%|███████████████████████████████| 269258/269258 [00:02<00:00, 104172.55it/s]\r\n",
      "15062\r\n",
      "100%|███████████████████████████████| 264330/264330 [00:01<00:00, 149835.56it/s]\r\n",
      "15942\r\n",
      "100%|███████████████████████████████| 263263/263263 [00:01<00:00, 148606.19it/s]\r\n",
      "15995\r\n",
      "total: 154892  tuple_sent_amr_level\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "Some weights of MBartForConditionalGeneration were not initialized from the model checkpoint at indobenchmark/indobart and are newly initialized: ['model.encoder.tree_embed.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  154892\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  38723\r\n",
      "(Epoch 1) TRAIN LOSS:1.0924 LR:0.00003000: 100%|█| 38723/38723 [1:04:49<00:00,  \r\n",
      "(Epoch 1) DEV LOSS:3.2416 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 42.12it/s]\r\n",
      "bleu score on dev:  11.679297471074166\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:17<00:00,  4.30it/s]\r\n",
      "sample:  ilham meniupkan balon. ---- balon itu ditiup ilham\r\n",
      "sample:  obe menulis puisi. ---- obe menulis puisi\r\n",
      "sample:  saya ketik makalahnya. ---- saya mengetik makalah\r\n",
      "sample:  angga, anak ajaib. ---- angga anak ajaib\r\n",
      "sample:  ibu saya orang dosen. ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  33.7612661376762\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x train_indoBART_on_silver_data.sh\n",
    "!./train_indoBART_on_silver_data.sh linearized_penman_with_tree_level ../amr-indo-dataset/data_154k_raw_silver_amr_indo4b_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "058bc0c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T02:34:33.821121Z",
     "iopub.status.busy": "2022-05-24T02:34:33.820375Z",
     "iopub.status.idle": "2022-05-24T02:38:04.427843Z",
     "shell.execute_reply": "2022-05-24T02:38:04.427192Z"
    },
    "papermill": {
     "duration": 245.642659,
     "end_time": "2022-05-24T02:38:04.427998",
     "exception": false,
     "start_time": "2022-05-24T02:33:58.785339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "resume from checkpoint\r\n",
      "added 9 tokens\r\n",
      "len train dataset:  2648\r\n",
      "len dev dataset:  19\r\n",
      "len test dataset: 306\r\n",
      "len train dataloader:  662\r\n",
      "(Epoch 1) TRAIN LOSS:0.9211 LR:0.00003000: 100%|█| 662/662 [00:45<00:00, 14.66it\r\n",
      "(Epoch 1) DEV LOSS:0.9632 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 42.63it/s]\r\n",
      "bleu score on dev:  32.96334344253099\r\n",
      "(Epoch 2) TRAIN LOSS:0.4920 LR:0.00003000: 100%|█| 662/662 [00:46<00:00, 14.38it\r\n",
      "(Epoch 2) DEV LOSS:1.0497 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 43.43it/s]\r\n",
      "bleu score on dev:  38.07291096974595\r\n",
      "(Epoch 3) TRAIN LOSS:0.3933 LR:0.00003000: 100%|█| 662/662 [00:45<00:00, 14.70it\r\n",
      "(Epoch 3) DEV LOSS:0.7138 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 43.58it/s]\r\n",
      "bleu score on dev:  49.01011729664867\r\n",
      "(Epoch 4) TRAIN LOSS:0.3588 LR:0.00003000: 100%|█| 662/662 [00:45<00:00, 14.62it\r\n",
      "(Epoch 4) DEV LOSS:0.7663 LR:0.00003000: 100%|████| 5/5 [00:00<00:00, 26.77it/s]\r\n",
      "bleu score on dev:  40.23999304844753\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.18it/s]\r\n",
      "sample:  balon dipukul oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  badannya anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "bleu score on test dataset:  32.92429801457668\r\n"
     ]
    }
   ],
   "source": [
    "!python train_indoBART.py --model_type indo-bart --n_epochs 4 --lr 3e-5 --num_beams 10 --data_folder ../data/preprocessed_data/linearized_penman_with_tree_level  --result_folder result --resume_from_checkpoint True --saved_model_folder_path result/result_supervised_task_adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f12d67a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T02:39:16.379631Z",
     "iopub.status.busy": "2022-05-24T02:39:16.378744Z",
     "iopub.status.idle": "2022-05-24T02:41:14.809322Z",
     "shell.execute_reply": "2022-05-24T02:41:14.809748Z"
    },
    "papermill": {
     "duration": 154.464646,
     "end_time": "2022-05-24T02:41:14.809936",
     "exception": false,
     "start_time": "2022-05-24T02:38:40.345290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model folder result\r\n",
      "preprocessing method linearized_penman_with_tree_level\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data’: File exists\r\n",
      "preprocess amr_simple_test\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/amr_simple_test’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/amr_simple_test/test.amr.txt', result_sent_path='data/test/preprocessed_data/amr_simple_test/test.sent.txt', source_file_path='data/test/amr_simple_test.txt', source_folder_path=None)\r\n",
      "100%|███████████████████████████████████| 2618/2618 [00:00<00:00, 184076.04it/s]\r\n",
      "total: 306  tuple_sent_amr_level\r\n",
      "preprocess b-salah-darat\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/b-salah-darat’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/b-salah-darat/test.amr.txt', result_sent_path='data/test/preprocessed_data/b-salah-darat/test.sent.txt', source_file_path='data/test/b-salah-darat.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 676/676 [00:00<00:00, 129782.10it/s]\r\n",
      "total: 32  tuple_sent_amr_level\r\n",
      "preprocess c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/c-gedung-roboh’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/c-gedung-roboh/test.amr.txt', result_sent_path='data/test/preprocessed_data/c-gedung-roboh/test.sent.txt', source_file_path='data/test/c-gedung-roboh.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 606/606 [00:00<00:00, 127815.96it/s]\r\n",
      "total: 29  tuple_sent_amr_level\r\n",
      "preprocess d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/d-indo-fuji’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/d-indo-fuji/test.amr.txt', result_sent_path='data/test/preprocessed_data/d-indo-fuji/test.sent.txt', source_file_path='data/test/d-indo-fuji.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 745/745 [00:00<00:00, 122094.19it/s]\r\n",
      "total: 27  tuple_sent_amr_level\r\n",
      "preprocess f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/f-bunuh-diri’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/f-bunuh-diri/test.amr.txt', result_sent_path='data/test/preprocessed_data/f-bunuh-diri/test.sent.txt', source_file_path='data/test/f-bunuh-diri.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 468/468 [00:00<00:00, 134226.91it/s]\r\n",
      "total: 23  tuple_sent_amr_level\r\n",
      "preprocess g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘data/test/preprocessed_data/g-gempa-dieng’: File exists\r\n",
      "Namespace(mode='linearized_penman_with_tree_level', result_amr_path='data/test/preprocessed_data/g-gempa-dieng/test.amr.txt', result_sent_path='data/test/preprocessed_data/g-gempa-dieng/test.sent.txt', source_file_path='data/test/g-gempa-dieng.txt', source_folder_path=None)\r\n",
      "100%|█████████████████████████████████████| 412/412 [00:00<00:00, 129055.51it/s]\r\n",
      "total: 19  tuple_sent_amr_level\r\n",
      "evaluate on data amr_simple_test\r\n",
      "mkdir: cannot create directory ‘result/amr_simple_test’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 306\r\n",
      "  0%|                                                    | 0/77 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|███████████████████████████████████████████| 77/77 [00:12<00:00,  6.22it/s]\r\n",
      "sample:  balon dipukul oleh ilham ---- balon itu ditiup ilham\r\n",
      "sample:  puisi ditulis oleh obe ---- obe menulis puisi\r\n",
      "sample:  saya sedang mengetik makalah ---- saya mengetik makalah\r\n",
      "sample:  badannya anak yang ajaib ---- angga anak ajaib\r\n",
      "sample:  ibu seorang dosen ---- ibuku seorang dosen\r\n",
      "sample:  dia sedang berenang ---- dia sedang berenang\r\n",
      "sample:  buku yang hilang dicari oleh ilham ---- ilham sedang mencari bukunya yang hilang\r\n",
      "sample:  di halaman rumah, ibu sedang menyapu ---- ibu sedang menyapu halaman rumah\r\n",
      "sample:  padi yang ayah bajak di sawah ---- ayah sedang membajak padi di sawah\r\n",
      "sample:  di tepi pantai, andi pergi ---- andi pergi berkemah di tepi pantai\r\n",
      "bleu score on test dataset:  32.92429801457668\r\n",
      "evaluate on data b-salah-darat\r\n",
      "mkdir: cannot create directory ‘result/b-salah-darat’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 32\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:04<00:00,  1.83it/s]\r\n",
      "sample:  kantor kantor sama pt angkasa pura ii otoritas bandara soekarnohat dan imigrasi bandara soekarno-hatta membahas insiden tumpang terbang pesawat yang tiba di jakarta dari singapura dari 10 mei 2016 di lion air jt 161 ---- kantor imigrasi bandara soekarno-hatta bersama kantor otoritas bandara soekarno-hatta dan pt angkasa pura ii membahas insiden penumpang penerbangan pesawat lion air jt 161 yang tiba di jakarta dari singapura 10 mei 2016.\r\n",
      "sample:  kepala kantor imigrasi soekarnohatta alif suadi berkata, kami meeting soal kepada tempo pada sabtu malam ini (14 mei) 2016 ---- kami meeting soal ini,\" ujar kepala kantor imigrasi soekarno-hatta alif suadi kepada tempo, sabtu malam ini, 14 mei 2016.\r\n",
      "sample:  anak bersama temannya berangkat dan menggunakan pesawat lion air jt 161 pukul 18 55 di singapura ---- anak temannya natalie berangkat dari singapura dan menggunakan pesawat lion air jt 161, pada pukul 18.50.\r\n",
      "sample:  kata sekretaris agus haryadi, perusahaan pt angkasa pura ii akan melakukan koordinasi pengelolaan bandara internasional soekarnohatta dengan pihak yang akan mengkoordinasi kantor otoritas bandara wilayah 1/1 dalam kaitan perisitiwa ---- sekretaris perusahaan pt angkasa pura ii agus haryadi mengatakan selaku pengelola bandara internasional soekarno-hatta, pihaknya akan berkoordinasi dengan kantor otoritas bandara wilayah i terkait dengan peristiwa ini.\r\n",
      "sample:  di remote area, kami menyiapkan bus antar penumpang ke terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "sample:  di padang, kata edward, terdapat sopir yang menjemput penumpang ---- terdapat sopir bus yang akan menjemput penumpang dari padang\" ujar edward.\r\n",
      "sample:  pihak otoritas bandara soetta memanggil pihak terkait dan melakukan investigasi ---- pihak otoritas bandara soetta akan memanggil pihak terkait dan dilakukan investigasi.\r\n",
      "sample:  syifa memberikan sanksi kepadanya kepada pihak-pihak yang lalai ---- kemenhub memberikan sanksi kepada pihak-pihak yang lalai,\" ujarnya.\r\n",
      "sample:  zara berkata mengutip cerita temannya ketika pesawat itu tidak mendarat di terminal ii ---- pesawat itu mendarat tidak di terminal ii, kata zara mengutip cerita temannya.\r\n",
      "sample:  di remote area, kami menyiapkan bus antar penumpang ke terminal ---- setiap pesawat yang parkir di remote area sudah kami siapkan bus untuk mengantar penumpang ke terminal.\r\n",
      "bleu score on test dataset:  23.020192345291363\r\n",
      "evaluate on data c-gedung-roboh\r\n",
      "mkdir: cannot create directory ‘result/c-gedung-roboh’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 29\r\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.16it/s]\r\n",
      "sample:  pengalihan kendaraan melaju ke arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dilakukan dengan menggunakan flyover di atas lampu indomobil merah ---- pengalihan dilakukan bagi pengendara yang melaju dari arah mal bintaro xchange dan tol bintaro menuju jalan boulevard bintaro dengan menggunakan flyover di atas lampu merah indomobil.\r\n",
      "sample:  kegagalan dan kegagalannya memperbaiki struktur di gedung miring terjadi di menara jalan jakarta timur mt haryono dan saidah ---- kegagalan struktur dan kesalahan tes tanah yang mengakibatkan gedung miring juga terjadi pada menara saidah di jalan mt haryono, jakarta timur.\r\n",
      "sample:  kapolres ayi supardan akbp di tangerang selatan membenarkan peristiwa di bintaro yang roboh gedung itu menarik hati masyarakat banyak ---- kapolres tangerang selatan, akbp ayi supardan membenarkan peristiwa robohnya gedung di bintaro yang menarik perhatian masyarakat banyak.\r\n",
      "sample:  panin memutuskan untuk melanjutkan pembangunan gedung itu dikarenakan tidak lulus ujian yang layak ---- panin memutuskan tidak melanjutkan pembangunan gedung itu karena dinyatakan tidak lulus uji kelayakan.\r\n",
      "sample:  gedung itu dinyatakan tidak lulus ujian sehingga pembangunan yang memutuskan untuk tidak melanjutkan gedung ---- gedung tersebut dinyatakan tidak lulus uji sehingga pembangunan gedung diputuskan tidak dilanjutkan.\r\n",
      "sample:  kasat akp samian dari polres tangerang selatan mengatakan, saat ini pihaknya akan melakukan penyelidik dan mengetahui penyebab runtuhnya gedung itu ---- kasat reskrim polres tangerangg selatan, akp samian mengatakan saat ini pihaknya melakukan penyelidikan untuk mengetahui penyebab runtuhnya gedung.\r\n",
      "sample:  kami sedang mendalami peristiwa yang disebut sampai ini ---- hingga kini kami masih mendalami peristiwa tersebut.\r\n",
      "sample:  kejadian tersebut tidak menimbulkan korban ---- kejadian tersebut tidak timbul korban.\r\n",
      "sample:  terjadi kerugian material ---- terjadi kerugian material.\r\n",
      "sample:  di sektor bintaro jaya selatan, gedung tersebut diduga menyalahi prosedur ---- pembongkaran gedung di sektor 7, bintaro, tangerang selatan, diduga menyalahi prosedur.\r\n",
      "bleu score on test dataset:  22.077147149974888\r\n",
      "evaluate on data d-indo-fuji\r\n",
      "mkdir: cannot create directory ‘result/d-indo-fuji’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 27\r\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:04<00:00,  1.73it/s]\r\n",
      "sample:  indosat indonesia menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra pembangunan yang telah dan langgan bisnis dalam rangka menghadirkan solusi smart dan internet of things dalam mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  indosat indonesia menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra pembangunan yang telah dan langgan bisnis dalam rangka menghadirkan solusi smart dan internet of things dalam mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah dibangun dalam menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  teknologi mobility dan iot yang berkembang cukup signifikan dan peserta perubahan cara yang mendalam dalam pengelolaan aset ---- smart mobility dan iot mengalami perkembangan cukup signifikan sehingga mengubah cara perusahaan dalam mengelola aset.\r\n",
      "sample:  indosat indonesia menandatangani nota kesepahaman mendalam dalam rangka memantapkan mitra pembangunan yang telah dan langgan bisnis dalam rangka menghadirkan solusi smart dan internet of things dalam mobility ---- indosat ooredoo bersama fujitsu indonesia menandatangani nota kesepahaman dalam rangka memantapkan kemitraan dengan para pelanggan bisnis yang telah menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  kerja sama itu terfokus pada sektor transportasi dan korporasi di indonesia dan sektor industri bagai terpenuhi dan langgan korporasi hadir ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  president achmad sofwan director fujitsu indonesia menuturkan, layanan yang memproses data realtime dan mobilitasnya memanfaatkan cara yang optimal dalam strategi perusahaan ---- mobilitas akan menyediakan cara untuk mengoptimalkan strategi perusahaan dengan memanfaatkan aplikasi dan layanan yang dapat memproses data real-time, tutur achmad sofwan, president director fujitsu indonesia.\r\n",
      "sample:  kerja sama itu terfokus pada sektor transportasi dan korporasi di indonesia dan sektor industri bagai terpenuhi dan langgan korporasi hadir ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "sample:  indosat ooredoo bekerja untuk menghadiri solusi smart mobility internet of things dan bekerja sama dengan fujitsu di indonesia ---- indosat ooredoo bekerja sama dengan fujitsu indonesia menghadirkan solusi smart mobility dan internet of things.\r\n",
      "sample:  president achmad sofwan mengungkapkan director fujitsu bersedia memanfaatkan layanan yang memproses data realtime dan mobilitasnya dengan cara optimal strategi perusahaan ---- president director fujitsu, achmad sofwan mengungkap, mobilitas akan menyediakan cara mengoptimalisasikan strategi perusahaan dengan memanfaatkan aplikasi dan layanan dalam aktivitas memproses data real-time.\r\n",
      "sample:  kerja sama itu terfokus pada sektor transportasi dan korporasi di indonesia dan sektor industri bagai terpenuhi dan langgan korporasi hadir ---- kerja sama ini akan berfokus pada sektor otomotif dan transportasi dan akan diperluas ke berbagai sektor industri untuk memenuhi kebutuhan pelanggan korporasi di indonesia.\r\n",
      "bleu score on test dataset:  11.493968374149238\r\n",
      "evaluate on data f-bunuh-diri\r\n",
      "mkdir: cannot create directory ‘result/f-bunuh-diri’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 23\r\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:03<00:00,  1.87it/s]\r\n",
      "sample:  cangkangron melompat di jembatan yang tinggi diperkirakan mencapai 15 meter sebrang orang ---- ciptadi dengan nekat melompat dari megatron jembatan penyeberangan orang yang tingginya diperkirakan mencapai 15 meter.\r\n",
      "sample:  hal yang disebutkan oleh kasubag reny marthaliana kepada humas kompol damping kabag dede rojudin kompol opsudin di bandung, jumat ini terungkap di mapolrestabes bandung ---- hal tersebut diungkapkan kabag ops polrestabes bandung kompol dede rojudin didampingi kasubag humas kompol reny marthaliana, saat ditemui di mapolrestabes bandung, jumat.\r\n",
      "sample:  rezha warga kota bandung mengatakan telah membubarkan salat di masjid raya bandung jumat dan ia dan jemaat lainnya di depan kantor di bandung di pos besar di depan jembatan penyeberangan orang ---- rezha noviana, warga kota bandung mengatakan ia dan jemaat lainnya langsung memperhatikan megatron jembatan penyeberangan orang depan kantor pos besar bandung setelah bubar salat jumat di masjid raya bandung.\r\n",
      "sample:  petugas linmas berdiri dan berapa satpam berdiri di atas jpo ---- ketika itu, petugas linmas dan beberapa satpam berdiri di atas jpo.\r\n",
      "sample:  aksi bunuh diri terjadi di alun bandung alun belum lama ---- aksi bunuh diri di alun - alun bandung belum lama ini terjadi.\r\n",
      "sample:  di alun kota bandung, orang tua terjatuh oleh dirinya jpo ---- seorang pria tua nekat menjatuhkan diri dari jpo di alun - alun kota bandung.\r\n",
      "sample:  sudirman yang menjadi saksi mata pembunuhan di alun bandung, mengatakan dirinya seorang anggota linmas di kota bandung dari satpol pp, pria yang disebutnya memperkirakan 30 tahun ---- sudirman, anggota linmas satpol pp kota bandung yang menjadi saksi mata aksi bunuh diri di alun - alun bandung itu mengatakan, pria tersebut diperkirakan berusia 50 tahun.\r\n",
      "sample:  di depan bandung, seorang lelaki jatuh diri di pos giro besar bandung di jembatan seberang, jumat ini ---- seorang lelaki menjatuhkan diri dari megatron jembatan penyeberangan di depan pos giro besar bandung, bandung, jumat.\r\n",
      "sample:  di tengah baya, hidup diakhiri oleh lelaki itu di tengah jalan asia afrika yang lalu lintas ---- seorang lelaki tengah baya nekat mengakhiri hidupnya di tengah kepadatan lalu lintas jalan asia afrika.\r\n",
      "sample:  pesanan dibuat oleh pria misterius yang bunuh diri di bandung, jawa barat, asia afrika, di seberang orang-orang belum terjun nekat ---- pria misterius yang bunuh diri dari jembatan penyeberangan orang di jalan asia afrika, bandung, jawa barat, membuat pesan sebelum nekat terjun.\r\n",
      "bleu score on test dataset:  22.229461622657976\r\n",
      "evaluate on data g-gempa-dieng\r\n",
      "mkdir: cannot create directory ‘result/g-gempa-dieng’: File exists\r\n",
      "Running on the GPU\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\r\n",
      "added 9 tokens\r\n",
      "PreTrainedTokenizer(name_or_path='indobenchmark/indobart', vocab_size=40004, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[java]', '[sunda]', '[indonesia]', '<mask>', ':arg0', ':arg1', ':mod', ':time', ':name', ':location', ':op1', ':op2', ':root']})\r\n",
      "MBartConfig {\r\n",
      "  \"_name_or_path\": \"result/model\",\r\n",
      "  \"activation_dropout\": 0.1,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"add_bias_logits\": false,\r\n",
      "  \"add_final_layer_norm\": false,\r\n",
      "  \"additional_config\": {\r\n",
      "    \"tree_max\": 512\r\n",
      "  },\r\n",
      "  \"architectures\": [\r\n",
      "    \"MBartForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classif_dropout\": 0.1,\r\n",
      "  \"classifier_dropout\": 0.0,\r\n",
      "  \"d_model\": 768,\r\n",
      "  \"decoder_attention_heads\": 12,\r\n",
      "  \"decoder_ffn_dim\": 3072,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 6,\r\n",
      "  \"decoder_start_token_id\": 2,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": true,\r\n",
      "  \"encoder_attention_heads\": 12,\r\n",
      "  \"encoder_ffn_dim\": 3072,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 6,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"forced_eos_token_id\": 2,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"max_position_embeddings\": 1024,\r\n",
      "  \"model_type\": \"mbart\",\r\n",
      "  \"no_repeat_ngram_size\": 3,\r\n",
      "  \"normalize_before\": false,\r\n",
      "  \"normalize_embedding\": true,\r\n",
      "  \"num_beams\": 4,\r\n",
      "  \"num_hidden_layers\": 6,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 128,\r\n",
      "      \"min_length\": 12,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_cnn\": {\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 142,\r\n",
      "      \"min_length\": 56,\r\n",
      "      \"num_beams\": 4\r\n",
      "    },\r\n",
      "    \"summarization_xsum\": {\r\n",
      "      \"length_penalty\": 1.0,\r\n",
      "      \"max_length\": 62,\r\n",
      "      \"min_length\": 11,\r\n",
      "      \"num_beams\": 6\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"tokenizer_class\": \"IndoNLGTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 40016\r\n",
      "}\r\n",
      "\r\n",
      "len test dataset: 19\r\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:02<00:00,  1.95it/s]\r\n",
      "sample:  stasiun di banjarnegara, jawa tengah, badan meteorologi geofisika menyatakan, gempa berkekuatan 4,8 skala richter mengguncang dataran tinggi dieng pada pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara, jawa tengah, menyatakan bahwa gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 pada skala richter.\r\n",
      "sample:  di kecamatan kabupaten batang, gempa diperkirakan ada di desa tanji gugur ---- pusat gempa diperkirakan berada di desa tanji gugur, kecamatan bawang, kabupaten batang.\r\n",
      "sample:  pvg merekomendasikan untuk menutup sementara jalan yang dekat ke kawah di desa sumberejo secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  kepala pusat vulkanologi mitigasi bencana geologi mengatakan terdapat 86 kali gempa yang terjadi pukul 19 00 wib di dataran tinggi dieng ---- kepala pusat vulkanologi dan mitigasi bencana geologi surono mengatakan bahwa gempa yang terjadi di dataran tinggi dieng pukul 19.00 wib terekam sebanyak 86 kali.\r\n",
      "sample:  penutupan dilakukan hingga tim yang mengukur konsentrasi gas di lapangan menyatakan tidak ada gas berbahaya ---- penutupan ini dilakukan hingga tim di lapangan yang mengukur konsentrasi gas menyatakan tidak ada gas yang berbahaya.\r\n",
      "sample:  pvg merekomendasikan untuk menutup sementara jalan yang dekat ke kawah di desa sumberejo secara keseluruhan ---- pvmbg merekomendasikan seluruh jalan yang mendekati kawah timbang di desa sumberejo sementara ditutup.\r\n",
      "sample:  petugas pusat vulkanologi mitigasi bencana geologi menerimaku untuk memantau aktivitas kawah timbang dan memberikan informasi ---- petugas pusat vulkanologi dan mitigasi bencana geologi diterjunkan untuk memantau aktivitas kawah timbang dan memberikan informasi.\r\n",
      "sample:  data yang dikeluarkan pusat vulkanologi mitigasi bencana geologi menyebutkan dan telah merekam gempa sebanyak 86 kali sejak pukul 19 00 wib ---- data yang dikeluarkan dari pusat vulkanologi dan mitigasi bencana geologi menyebutkan gempa mulai terasa pukul 19.00 wib dan telah terekam gempa sebanyak 86 kali.\r\n",
      "sample:  pengungsi di kabupaten banjarnegara meninggalkan pengungsi pada sabtu pagi ---- pengungsi di kabupaten banjarnegara, pada sabtu pagi mulai meninggalkan pengungsian.\r\n",
      "sample:  stasiun banjarnegara menyatakan badan meteorologi geofisika diguncang gempa berkekuatan 4,8 skala richter di dataran tinggi dieng pada pukul 19 00 wib ---- stasiun geofisika badan meteorologi klimatologi dan geofisika banjarnegara menyatakan gempa yang mengguncang dataran tinggi dieng pukul 19.00 wib berkekuatan 4,8 skala richter *(sr)*.\r\n",
      "bleu score on test dataset:  27.024355730068482\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x ./evaluate_indoBART_to_all.sh\n",
    "!./evaluate_indoBART_to_all.sh result linearized_penman_with_tree_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e254dda1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T02:42:27.274859Z",
     "iopub.status.busy": "2022-05-24T02:42:27.274079Z",
     "iopub.status.idle": "2022-05-24T02:43:24.068214Z",
     "shell.execute_reply": "2022-05-24T02:43:24.066583Z"
    },
    "papermill": {
     "duration": 92.835974,
     "end_time": "2022-05-24T02:43:24.068433",
     "exception": false,
     "start_time": "2022-05-24T02:41:51.232459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/ (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_label.txt (deflated 68%)\r\n",
      "  adding: result/f-bunuh-diri/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/f-bunuh-diri/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/result_supervised_task_adaptation/ (stored 0%)\r\n",
      "  adding: result/result_supervised_task_adaptation/test_label.txt (deflated 60%)\r\n",
      "  adding: result/result_supervised_task_adaptation/loss_data.tsv (deflated 3%)\r\n",
      "  adding: result/result_supervised_task_adaptation/model/ (stored 0%)\r\n",
      "  adding: result/result_supervised_task_adaptation/model/config.json (deflated 63%)\r\n",
      "  adding: result/result_supervised_task_adaptation/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/result_supervised_task_adaptation/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/result_supervised_task_adaptation/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/c-gedung-roboh/ (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_label.txt (deflated 62%)\r\n",
      "  adding: result/c-gedung-roboh/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/c-gedung-roboh/test_generations.txt (deflated 61%)\r\n",
      "  adding: result/test_label.txt (deflated 60%)\r\n",
      "  adding: result/amr_simple_test/ (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_label.txt (deflated 60%)\r\n",
      "  adding: result/amr_simple_test/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/amr_simple_test/test_generations.txt (deflated 63%)\r\n",
      "  adding: result/loss_data.tsv (deflated 33%)\r\n",
      "  adding: result/model/ (stored 0%)\r\n",
      "  adding: result/model/config.json (deflated 63%)\r\n",
      "  adding: result/model/pytorch_model.bin (deflated 8%)\r\n",
      "  adding: result/b-salah-darat/ (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_label.txt (deflated 66%)\r\n",
      "  adding: result/b-salah-darat/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/b-salah-darat/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/d-indo-fuji/ (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_label.txt (deflated 86%)\r\n",
      "  adding: result/d-indo-fuji/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/d-indo-fuji/test_generations.txt (deflated 83%)\r\n",
      "  adding: result/g-gempa-dieng/ (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_label.txt (deflated 67%)\r\n",
      "  adding: result/g-gempa-dieng/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/g-gempa-dieng/test_generations.txt (deflated 64%)\r\n",
      "  adding: result/bleu_score_test.txt (stored 0%)\r\n",
      "  adding: result/test_generations.txt (deflated 63%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r result_with_sta.zip result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9406.941907,
   "end_time": "2022-05-24T02:44:03.838760",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-24T00:07:16.896853",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
